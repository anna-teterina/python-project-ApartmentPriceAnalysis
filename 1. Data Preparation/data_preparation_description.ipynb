{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this file step by step are described the transformations of the data made in the files:\n",
    "* pipeline_pre-processing.py\n",
    "* pipeline_for_training_data.ipynb\n",
    "* model-building.ipynb\n",
    "* pipeline_for_production.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import geocoder\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "import folium\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "from IPython.display import display\n",
    "\n",
    "import joblib\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=False)\n",
    "\n",
    "work_dir = r'C:\\Users\\User\\Desktop\\python-project-ApartmentPriceAnalysis'\n",
    "os.chdir(work_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_initial = pd.read_csv('data_2024-01.csv', index_col=0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data description\n",
    "The data includes the following information:\n",
    "1. **link** - link to the ad\n",
    "2. **price** - the price in PLN given in the ad or \"Zapytaj o cenę\" (\"Ask price\") in case of no price given\n",
    "3. **address** - the addres given in the ad\n",
    "4. **area** - apartment area in m²\n",
    "5. **num_rooms** - number of rooms in the apartment\n",
    "6. **floor** - the floor on which the apartment is located, usually given in the form of floor by the number of floors in the entire building, for example, 1/7\n",
    "7. **rent** - monthly rent value in PLN\n",
    "8. **ownership_status** - form of ownership of the apartment, in Poland there are full ownership, cooperative ownership right to premises, cooperative tenant right to premises and right to municipal premises\n",
    "9. **flat_condition** - condition of the apartment (to be moved in/to be finished/to be renovated)\n",
    "10. **perks** - information whether the apartment has a balcony, garden or terrace\n",
    "11. **parking** - whether the apartment has parking space\n",
    "12. **heating** - type of heating of the apartment (municipal/gas/electric/boiler room/tiled stoves/other)\n",
    "13. **market** - primary or secondary market\n",
    "14. **ad_type** - advertiser type (real estate office/developer/private)\n",
    "15. **availability** - date from when the apartment is available\n",
    "16. **year** - year of building\n",
    "17. **devel_type** - building type (Apartment block/Condominium/Townhouse/Row house etc.)\n",
    "18. **windows** - material of windows in the apartment(plastic/wooden/aluminum)\n",
    "19. **lift** - whether the apartment building has an elevator\n",
    "20. **mater** - apartment building material (brick/hollow block/silicate/large slab etc.)\n",
    "21. **utilities** - whether the apartment has Internet, cable TV, telephone\n",
    "22. **security** - whether the apartment has intercom, video intercom, territory monitoring etc.\n",
    "23. **equipment** - whether the apartment has dishwasher, refrigerator, furniture, oven etc.\n",
    "24. **add_inf** - additional information, for example, whether the apartment has air conditioning, basement, separate kitchen etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach to missing data\n",
    "In the scrapped data missing values were marked marked with \"Zapytaj o cenę\" (\"Ask price\"), \"Zapytaj\" (\"Ask\") and \"brak informacji\" (\"no information\")<br>\n",
    "Variables **'address'**, **'area'**, **'num_rooms'**, **'lift'**, **'market'** and **'ad_type'** had no missing data.<br><br>\n",
    "The next approach to missing data was applied:\n",
    "1. Variables with too much missing data (above 80%) were removed (**'availability'**)\n",
    "2. Observations with missing key variables were removed (**'devel_type'**)\n",
    "3. For categorical variables **'ownership_status'**, **'flat_condition'**, **'heating'**, **'windows'**, **'mater'**, a separate category named \"nie podano\" (\"not provided\") was created for missing data\n",
    "4. No missing data were filled for variables **'parking'**, **'perks'**, **'utilities'**, **'security'**, **'equipment'**, **'add_inf'**. It was recognized that in the case of missing data, the apartment has nothing of the facilities described in the variable\n",
    "5. For numerical variables, the gaps were filled using the KNN method (**'rent_cat'**, **'year'**, **'floor'**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing data transformation\n",
    "Used in file: **pipeline_pre-processing.py**<br>\n",
    "In the file above, a function with data preprocessing is defined.<br>\n",
    "It consists of such functions as:\n",
    "1. **standardize_missing_values** - Converts placeholders like \"Zapytaj o cenę\" (\"Ask price\") to NaN\n",
    "2. **clean_numeric_columns** - Cleans and converts numeric columns such as 'price', 'area', 'rent'\n",
    "3. **categorize_rent** - Categorizes rental prices into bins\n",
    "4. **process_floor_data** - Splits and normalizes apartment floor info\n",
    "5. **fill_missing_categoricals** - Fills missing categorical values with \"nie podano\" (\"not provided\")\n",
    "6. **encode_parking_presence** - Encodes binary presence of parking (e.g., yes/no)\n",
    "7. **convert_year_to_int** - Converts the 'year' column is of integer type \n",
    "8. **standardize_ownership_labels** - Standardizes ownership labels (e.g., unifying similar terms)\n",
    "9. **multiple_choice_transform** - One-hot encodes multi-choice variables based on a predefined dictionary\n",
    "10. **location_transform** - Extracts region, city, and street from the address\n",
    "11. **city_info_transform** - Adds population and administrative info by merging with external city data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### standardize_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_missing_values(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Replaces custom placeholders for missing values with standard NaN values.\n",
    "\n",
    "    This function searches the DataFrame for specific strings that are used\n",
    "    to indicate missing or unavailable information (e.g., 'Zapytaj o cenę',\n",
    "    'Zapytaj', 'brak informacji') and replaces them with `np.NaN`, which is\n",
    "    the standard missing value marker in pandas.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The input DataFrame to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        A DataFrame with specified placeholder values replaced by NaN.\n",
    "    \"\"\"\n",
    "    missing_placeholders = [\n",
    "        'Zapytaj o cenę', # 'Ask price'\n",
    "        'Zapytaj',        # 'Ask'\n",
    "        'brak informacji' # 'no information'\n",
    "        ]\n",
    "    \n",
    "    return data.replace(missing_placeholders, np.NaN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  missing_count  missing_percent\n",
      "availability              38287            81.84\n",
      "equipment                 32486            69.44\n",
      "rent                      25514            54.54\n",
      "utilities                 21380            45.70\n",
      "parking                   20529            43.88\n",
      "add_inf                   19672            42.05\n",
      "windows                   16767            35.84\n",
      "mater                     16710            35.72\n",
      "security                  14992            32.05\n",
      "perks                     10959            23.43\n",
      "ownership_status           8158            17.44\n",
      "flat_condition             7465            15.96\n",
      "heating                    6028            12.89\n",
      "price                      2109             4.51\n",
      "floor                       751             1.61\n",
      "year                         26             0.06\n",
      "devel_type                    9             0.02\n",
      "lift                          0             0.00\n",
      "link                          0             0.00\n",
      "ad_type                       0             0.00\n",
      "num_rooms                     0             0.00\n",
      "area                          0             0.00\n",
      "address                       0             0.00\n",
      "market                        0             0.00\n"
     ]
    }
   ],
   "source": [
    "# Application of the function\n",
    "standardized_missing_values_data = standardize_missing_values(data_initial)\n",
    "\n",
    "# View the changes that have made to the data\n",
    "missing_info = (\n",
    "    standardized_missing_values_data.isna()\n",
    "    .sum()\n",
    "    .to_frame(name='missing_count')\n",
    "    .assign(\n",
    "        missing_percent=lambda df: round(100 * df['missing_count'] / len(standardized_missing_values_data), 2)\n",
    "    )\n",
    "    .sort_values(by='missing_count', ascending=False)\n",
    ")\n",
    "\n",
    "print(missing_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean_numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numeric_columns(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Cleans and converts specified columns containing numeric values with extra characters to float type.\n",
    "\n",
    "    This function is designed to process the columns \"price\", \"area\", and \"rent\" in a DataFrame\n",
    "    where numeric values may be represented as strings containing non-numeric characters such as\n",
    "    units (e.g., \"m²\"), letters, or whitespace. The steps include:\n",
    "\n",
    "    1. Converting each value to string format to enable regex processing.\n",
    "    2. Removing all alphabetic characters (including Polish-specific letters like 'ł', 'Ł', '²') and spaces.\n",
    "    3. Replacing commas with dots to correctly format decimal numbers.\n",
    "    4. Converting cleaned strings to numeric (float) values using `pd.to_numeric`.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The input DataFrame that contains the columns \"price\", \"area\", and \"rent\".\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The modified DataFrame with \"price\", \"area\", and \"rent\" columns cleaned and converted to float.\n",
    "    \"\"\"\n",
    "    \n",
    "    for var in [\"price\", \"area\", \"rent\"]:\n",
    "        \n",
    "        data[var] = (\n",
    "            data[var]\n",
    "            .astype(str)                              \n",
    "            .str.replace('[ a-zA-ZłŁ²]*', '', regex=True)\n",
    "            .str.replace(',', '.', regex=False)\n",
    "        )\n",
    "        \n",
    "        data[var] = pd.to_numeric(data[var])\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   price_before area_before rent_before      price   area    rent\n",
      "0    415 000 zł     37,4 m²     Zapytaj   415000.0  37.40     NaN\n",
      "1    880 000 zł     68,5 m²      750 zł   880000.0  68.50   750.0\n",
      "2    590 000 zł       60 m²        1 zł   590000.0  60.00     1.0\n",
      "3    699 000 zł       77 m²     Zapytaj   699000.0  77.00     NaN\n",
      "4  1 378 000 zł    69,03 m²    1 120 zł  1378000.0  69.03  1120.0\n"
     ]
    }
   ],
   "source": [
    "# Application of the function\n",
    "cleaned_numeric_columns_data = clean_numeric_columns(standardized_missing_values_data)\n",
    "\n",
    "# View the changes that have made to the data\n",
    "clean_numeric_columns_show = pd.concat([data_initial[[\"price\", \"area\", \"rent\"]].head(),\n",
    "               cleaned_numeric_columns_data[[\"price\", \"area\", \"rent\"]].head()],\n",
    "              axis=1)\n",
    "\n",
    "clean_numeric_columns_show.columns = [\"price_before\", \"area_before\", \"rent_before\",\n",
    "                                      \"price\", \"area\", \"rent\"]\n",
    "print(clean_numeric_columns_show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### categorize_rent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_rent(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Categorizes rental prices into discrete bins and creates a new column 'rent_cat'.\n",
    "\n",
    "    This function groups the values from the 'rent' column into five predefined ranges (bins)\n",
    "    to simplify analysis or modeling. Each bin is assigned a numeric label from 1 to 5.\n",
    "    The bins are: \n",
    "        - 0 to 500\n",
    "        - 501 to 1000\n",
    "        - 1001 to 1500\n",
    "        - 1501 to 2000\n",
    "        - 2001 and above\n",
    "\n",
    "    The original 'rent' column is removed after categorization, and the new 'rent_cat'\n",
    "    column is added with float-typed values.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    data : pd.DataFrame\n",
    "        The input DataFrame that contains a 'rent' column with numeric values.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        The modified DataFrame with the 'rent' column replaced by a categorical 'rent_cat' column.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Group the values from the 'rent' column into ranges\n",
    "    data['rent_cat'] = pd.cut(data['rent'],\n",
    "                              bins = [0, 500, 1000, 1500, 2000, np.inf],\n",
    "                              labels = np.arange(1, 6, 1))\n",
    "    data['rent_cat'] = data['rent_cat'].astype(\"float\")\n",
    "    \n",
    "    # Drop original 'rent' column\n",
    "    data = data.drop('rent', axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  rent_before  rent_cat\n",
      "1      750 zł       2.0\n",
      "2        1 zł       1.0\n",
      "4    1 120 zł       3.0\n",
      "5      750 zł       2.0\n",
      "6      700 zł       2.0\n"
     ]
    }
   ],
   "source": [
    "# Application of the function\n",
    "categorized_rent_data = categorize_rent(cleaned_numeric_columns_data)\n",
    "\n",
    "# View the changes that have made to the data\n",
    "categorize_rent_show = pd.concat([data_initial[[\"rent\"]].head(7),\n",
    "                                  categorized_rent_data[[\"rent_cat\"]].head(7)],\n",
    "                                 axis=1)\n",
    "\n",
    "categorize_rent_show.columns = [\"rent_before\", \"rent_cat\"]\n",
    "print(categorize_rent_show.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### process_floor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_floor_data(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Process floor information in the dataset by extracting and standardizing floor-related features.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Extracts the total number of floors in the building from the 'floor' column.\n",
    "       - Assumes the format is 'apartment_floor/number_of_floors'.\n",
    "       - If the format does not contain '/', sets the value to NaN.\n",
    "    2. Extracts the apartment's floor number from the 'floor' column.\n",
    "       - Converts special floor names ('parter', 'suterena') to '0'.\n",
    "       - Replaces '> 10' with None (to handle inconsistent data).\n",
    "    3. If the apartment's floor is labeled as 'poddasze' (attic), replaces it with the total number of floors.\n",
    "    4. Converts the apartment floor and total floors columns to float type.\n",
    "    5. Removes the original 'floor' column from the dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    data : pandas.DataFrame\n",
    "        The input dataframe containing a 'floor' column with floor information.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The dataframe with two new columns:\n",
    "        - 'number_floor_in_building': total floors in the building as float.\n",
    "        - 'ap_floor': apartment floor number as float.\n",
    "        The original 'floor' column is dropped.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract apartment floor (left part before '/'), convert special names\n",
    "    data['number_floor_in_building'] = (\n",
    "        data['floor']\n",
    "        .apply(lambda x: str(x).split('/')[1] if str(x).__contains__('/') else np.NaN)\n",
    "        .astype('float')\n",
    "        )\n",
    "    data['ap_floor'] = data['floor'].apply(lambda x: str(x).split('/')[0]).replace({'parter':'0',\n",
    "                                                                                    'suterena':'0',\n",
    "                                                                                    '> 10': None})\n",
    "    # Replace 'poddasze' (attic) with total floors in building\n",
    "    data['ap_floor'] = np.where(data['ap_floor'] == 'poddasze',\n",
    "                                        data['number_floor_in_building'], \n",
    "                                        data['ap_floor'])\n",
    "    # Convert apartment floor to float\n",
    "    data['ap_floor'] = data['ap_floor'].astype('float')\n",
    "    \n",
    "    # Drop original 'floor' column\n",
    "    data = data.drop('floor', axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      floor  ap_floor  number_floor_in_building\n",
      "0       1/3       1.0                       3.0\n",
      "1       4/7       4.0                       7.0\n",
      "2  parter/1       0.0                       1.0\n",
      "3    parter       0.0                       NaN\n",
      "4       3/4       3.0                       4.0\n"
     ]
    }
   ],
   "source": [
    "# Application of the function\n",
    "processed_floor_data = process_floor_data(categorized_rent_data)\n",
    "\n",
    "# View the changes that have made to the data\n",
    "process_floor_data_show = pd.concat([data_initial[[\"floor\"]].head(),\n",
    "                                  processed_floor_data[[\"ap_floor\", \"number_floor_in_building\"]].head()],\n",
    "                                 axis=1)\n",
    "\n",
    "print(process_floor_data_show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fill_missing_categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_categoricals(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Fill missing values in selected categorical columns with a placeholder string.\n",
    "\n",
    "    This function targets a predefined list of categorical columns and replaces any\n",
    "    missing (NaN) values with the string 'nie podano' (Polish for 'not provided').\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    data : pandas.DataFrame\n",
    "        The input DataFrame containing the columns to process.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The DataFrame with missing values in the specified categorical columns\n",
    "        filled with the placeholder string.\n",
    "    \"\"\"\n",
    "    # single-choice variables in the data\n",
    "    cols = ['ownership_status', 'flat_condition', 'heating', 'windows', 'mater']\n",
    "    data[cols] = data[cols].fillna('nie podano') # 'not provided'\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          missing_count  missing_percent\n",
      "availability                      38287            81.84\n",
      "equipment                         32486            69.44\n",
      "rent_cat                          25514            54.54\n",
      "utilities                         21380            45.70\n",
      "parking                           20529            43.88\n",
      "add_inf                           19672            42.05\n",
      "security                          14992            32.05\n",
      "perks                             10959            23.43\n",
      "number_floor_in_building           2238             4.78\n",
      "price                              2109             4.51\n",
      "ap_floor                           1235             2.64\n",
      "year                                 26             0.06\n",
      "devel_type                            9             0.02\n",
      "ad_type                               0             0.00\n",
      "market                                0             0.00\n",
      "windows                               0             0.00\n",
      "lift                                  0             0.00\n",
      "heating                               0             0.00\n",
      "flat_condition                        0             0.00\n",
      "ownership_status                      0             0.00\n",
      "num_rooms                             0             0.00\n",
      "mater                                 0             0.00\n",
      "area                                  0             0.00\n",
      "address                               0             0.00\n",
      "link                                  0             0.00\n"
     ]
    }
   ],
   "source": [
    "# Application of the function\n",
    "fill_missing_categoricals_data = fill_missing_categoricals(processed_floor_data)\n",
    "\n",
    "# View the changes that have made to the data\n",
    "missing_info = (\n",
    "    fill_missing_categoricals_data.isna()\n",
    "    .sum()\n",
    "    .to_frame(name='missing_count')\n",
    "    .assign(\n",
    "        missing_percent=lambda df: round(100 * df['missing_count'] / len(fill_missing_categoricals_data), 2)\n",
    "    )\n",
    "    .sort_values(by='missing_count', ascending=False)\n",
    ")\n",
    "\n",
    "print(missing_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encode_parking_presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "garaż/miejsce parkingowe    26253\n",
       "Zapytaj                     20529\n",
       "Name: parking, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# review of the data\n",
    "data_initial['parking'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the values “garage/parking space” and “Ask” appear in the set. As a simplification, it is assumed that the “Ask” values mean that no such parking space is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_parking_presence(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Encode the presence of parking information into a binary indicator column.\n",
    "\n",
    "    This function creates a new column 'parking_coded' where:\n",
    "    - 1 indicates that parking information is present (non-missing),\n",
    "    - 0 indicates that parking information is missing (NaN).\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    data : pandas.DataFrame\n",
    "        The input DataFrame containing the 'parking' column.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The DataFrame with an additional 'parking_coded' column representing\n",
    "        parking presence as a binary indicator.\n",
    "    \"\"\"\n",
    "    \n",
    "    data['parking_coded'] = data['parking'].apply(lambda x: 0 if pd.isna(x) else 1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    parking  parking_coded\n",
      "0  garaż/miejsce parkingowe              1\n",
      "1  garaż/miejsce parkingowe              1\n",
      "2  garaż/miejsce parkingowe              1\n",
      "3  garaż/miejsce parkingowe              1\n",
      "4                   Zapytaj              0\n",
      "5                   Zapytaj              0\n"
     ]
    }
   ],
   "source": [
    "# Application of the function\n",
    "encoded_parking_presence_data = encode_parking_presence(fill_missing_categoricals_data)\n",
    "\n",
    "# View the changes that have made to the data\n",
    "encode_parking_presence_show = pd.concat([data_initial[[\"parking\"]].head(6),\n",
    "                                  encoded_parking_presence_data[[\"parking_coded\"]].head(6)],\n",
    "                                 axis=1)\n",
    "print(encode_parking_presence_show)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert_year_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_year_to_int(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Convert the 'year' column to integer type, coercing invalid strings to NaN.\n",
    "\n",
    "    Uses vectorized conversion with pandas.to_numeric, converting\n",
    "    any non-convertible string to NaN, then casts to nullable integer dtype.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    data : pandas.DataFrame\n",
    "        DataFrame with a 'year' column to be converted.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with 'year' column converted to nullable integer dtype.\n",
    "    \"\"\"\n",
    "    \n",
    "    data['year'] = data['year'].astype('Int64')\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descrition before:\n",
      "count     46782\n",
      "unique      207\n",
      "top        2023\n",
      "freq       9691\n",
      "Name: year, dtype: object\n",
      "\n",
      "Descrition after:\n",
      "count    46756.000000\n",
      "mean      1998.217747\n",
      "std         83.953663\n",
      "min          1.000000\n",
      "25%       1983.000000\n",
      "50%       2020.000000\n",
      "75%       2023.000000\n",
      "max       2027.000000\n",
      "Name: year, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Application of the function\n",
    "converted_year_to_int_data = convert_year_to_int(encoded_parking_presence_data)\n",
    "\n",
    "# View the changes that have made to the data\n",
    "print(\"Descrition before:\")\n",
    "print(data_initial['year'].describe())\n",
    "print()\n",
    "print(\"Descrition after:\")\n",
    "print(converted_year_to_int_data['year'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### standardize_ownership_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_ownership_labels(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Standardize specific labels in the 'ownership_status' column.\n",
    "\n",
    "    This function replaces the label 'spółdzielcze własnościowe' with\n",
    "    the standardized label 'spółdzielcze wł. prawo do lokalu' in the\n",
    "    'ownership_status' column. All other values remain unchanged.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    data : pandas.DataFrame\n",
    "        Input DataFrame containing the 'ownership_status' column.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with updated 'ownership_status' labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    data['ownership_status'] = data['ownership_status'].apply(\n",
    "        lambda x: 'spółdzielcze wł. prawo do lokalu' if x == 'spółdzielcze własnościowe' else x\n",
    "    )\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts before:\n",
      "ownership_status                 \n",
      "pełna własność                       36790\n",
      "Zapytaj                               8158\n",
      "spółdzielcze wł. prawo do lokalu      1576\n",
      "udział                                 171\n",
      "użytkowanie wieczyste / dzierżawa       86\n",
      "spółdzielcze własnościowe                1\n",
      "dtype: int64\n",
      "\n",
      "Value counts  after:\n",
      "ownership_status                 \n",
      "pełna własność                       36790\n",
      "nie podano                            8158\n",
      "spółdzielcze wł. prawo do lokalu      1577\n",
      "udział                                 171\n",
      "użytkowanie wieczyste / dzierżawa       86\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Application of the function\n",
    "standardized_ownership_labels_data = standardize_ownership_labels(converted_year_to_int_data)\n",
    "\n",
    "# View the changes that have made to the data\n",
    "print(\"Value counts before:\")\n",
    "print(data_initial[['ownership_status']].value_counts())\n",
    "print()\n",
    "print(\"Value counts  after:\")\n",
    "print(standardized_ownership_labels_data[['ownership_status']].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multiple_choice_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utilities: ['telewizja kablowa', 'internet', 'telefon']\n",
      "security: ['drzwi / okna antywłamaniowe', 'teren zamknięty', 'domofon / wideofon', 'monitoring / ochrona', 'rolety antywłamaniowe', 'system alarmowy']\n",
      "equipment: ['zmywarka', 'lodówka', 'meble', 'piekarnik', 'kuchenka', 'pralka', 'telewizor']\n",
      "add_inf: ['pom. użytkowe', 'piwnica', 'dwupoziomowe', 'oddzielna kuchnia', 'klimatyzacja']\n",
      "perks: ['balkon', 'taras', 'ogródek']\n"
     ]
    }
   ],
   "source": [
    "def items_of_var(var, data = standardized_ownership_labels_data):   \n",
    "    item_list = []\n",
    "    for items in data[var]:\n",
    "        present = str(items).split(',')\n",
    "        present = [x.strip(' ') for x in present]\n",
    "        item_list.extend(present)\n",
    "\n",
    "    item_list = list(dict.fromkeys(item_list))\n",
    "    if 'nan' in item_list:\n",
    "        item_list.remove('nan')\n",
    "    return item_list\n",
    "\n",
    "perklist = items_of_var('perks')\n",
    "utilitylist = items_of_var('utilities')\n",
    "securitylist = items_of_var('security')\n",
    "equipmentlist = items_of_var('equipment')\n",
    "additionallist = items_of_var('add_inf')\n",
    "\n",
    "var_values_dict = {'utilities': utilitylist,\n",
    "                   'security': securitylist,\n",
    "                   'equipment': equipmentlist,\n",
    "                   'add_inf': additionallist,\n",
    "                   'perks': perklist}\n",
    "\n",
    "for var in var_values_dict:\n",
    "    print(f'{var}: {var_values_dict[var]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. Data Preparation/multiple_choice_var_dict.joblib']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving the dictionary\n",
    "var_values_dict_path = \"1. Data Preparation/multiple_choice_var_dict.joblib\"\n",
    "joblib.dump(var_values_dict, var_values_dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitcolumn(serieslike, colname, items, missing_categories):\n",
    "    \n",
    "    \"\"\"\n",
    "    Splits a string column into binary indicator columns for a list of expected items.\n",
    "\n",
    "    This function processes a string from a specified column in a Series-like object,\n",
    "    splits it by commas into a list of features, and maps the presence of each expected\n",
    "    item in that list to a binary value (1 if present, 0 if not). It also tracks any\n",
    "    unexpected values (not included in `items`) by appending them to the `missing_categories` list.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    serieslike : pandas.Series or pandas.DataFrame\n",
    "        The data structure containing the column to split.\n",
    "        \n",
    "    colname : str\n",
    "        The name of the column to process.\n",
    "\n",
    "    items : list of str\n",
    "        The list of expected categorical values to detect in the column.\n",
    "\n",
    "    missing_categories : list\n",
    "        A list that will be extended with unexpected values encountered during processing.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    pandas.Series\n",
    "        A Series of binary values (0 or 1) indicating the presence of each item from `items`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the value from the given column\n",
    "    input_value = serieslike[colname]\n",
    "    \n",
    "      # If the value is missing, return a Series of 0s (none of the items are present)\n",
    "    if pd.isna(input_value):\n",
    "        return pd.Series([0 for x in items])\n",
    "    \n",
    "    # Split the string into individual items using commas\n",
    "    present = input_value.split(',')\n",
    "\n",
    "    # Remove leading/trailing spaces\n",
    "    present = [x.strip(' ') for x in present]\n",
    "\n",
    "    # Filter out empty or very short items (likely noise)\n",
    "    present = [x for x in present if len(x) > 1]\n",
    "    \n",
    "    # Create a binary list indicating whether each expected item is present\n",
    "    presences = [1 if x in present else 0 for x in items]\n",
    "    \n",
    "    # Identify any new/unexpected items not in the `items` list\n",
    "    new_items = [x for x in present if x not in items]\n",
    "    if new_items:\n",
    "        missing_categories.extend(new_items) # Add them to the missing_categories list\n",
    "    \n",
    "    return pd.Series(presences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_choice_transform(data, train_dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    Transforms multiple-choice categorical variables into binary indicator columns.\n",
    "\n",
    "    This function applies one-hot encoding to columns containing multiple-choice values\n",
    "    (e.g., \"option1, option2\") based on predefined expected values stored in a dictionary\n",
    "    loaded from a joblib file. For each such variable, it creates new binary columns\n",
    "    indicating the presence of each expected value.\n",
    "\n",
    "    If unexpected (missing) categories are found in the data and `train_dataset` is True,\n",
    "    they are collected and printed as a warning.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas.DataFrame\n",
    "        The dataset containing the multiple-choice categorical variables.\n",
    "\n",
    "    train_dataset : bool\n",
    "        Flag indicating whether the function is applied on training data.\n",
    "        If True, the function will print information about any new, unexpected categories.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        The transformed DataFrame with multiple-choice variables expanded into binary columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the predefined dictionary mapping each multiple-choice column\n",
    "    # to the list of expected values (options)\n",
    "    var_values_dict = joblib.load(\"1. Data Preparation/multiple_choice_var_dict.joblib\")\n",
    "    \n",
    "    # List to collect unexpected (missing) categories encountered in the data\n",
    "    missing_categories = []\n",
    "    \n",
    "    for key in var_values_dict:\n",
    "        # Prepare names for the new binary columns\n",
    "        column_names = [key + '_' + x for x in var_values_dict[key]]\n",
    "        \n",
    "        # Apply the splitcolumn function row-wise, creating binary indicators\n",
    "        data[column_names] = data.apply(\n",
    "            splitcolumn,\n",
    "            args = (key, var_values_dict[key], missing_categories),\n",
    "            axis = 1\n",
    "        )\n",
    "    \n",
    "    # Drop the original multiple-choice columns after transformation\n",
    "    data = data.drop(var_values_dict, axis=1)\n",
    "    \n",
    "    # If this is the training dataset, report any unexpected categories found\n",
    "    if train_dataset:\n",
    "        if missing_categories:\n",
    "            category_counts = Counter(missing_categories)\n",
    "            print(\"There are new categories that are not in the dictionary:\")\n",
    "            for category, count in category_counts.items():\n",
    "                print(f\"  - {category}: {count} times\")\n",
    "        else:\n",
    "            print(\"All the categorizations occurring in the set in multi-vector selection variables were coded.\")\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the categorizations occurring in the set in multi-vector selection variables were coded.\n",
      "                              utilities  utilities_telewizja kablowa  \\\n",
      "0  telewizja kablowa, internet, telefon                            1   \n",
      "1           telewizja kablowa, internet                            1   \n",
      "2           telewizja kablowa, internet                            1   \n",
      "3                              internet                            0   \n",
      "4  telewizja kablowa, internet, telefon                            1   \n",
      "5  telewizja kablowa, internet, telefon                            1   \n",
      "\n",
      "                                            security  \\\n",
      "0  drzwi / okna antywłamaniowe, teren zamknięty, ...   \n",
      "1  drzwi / okna antywłamaniowe, teren zamknięty, ...   \n",
      "2                teren zamknięty, domofon / wideofon   \n",
      "3  drzwi / okna antywłamaniowe, teren zamknięty, ...   \n",
      "4  drzwi / okna antywłamaniowe, domofon / wideofo...   \n",
      "5                                 domofon / wideofon   \n",
      "\n",
      "   security_domofon / wideofon  \\\n",
      "0                            1   \n",
      "1                            1   \n",
      "2                            1   \n",
      "3                            1   \n",
      "4                            1   \n",
      "5                            1   \n",
      "\n",
      "                                           equipment  equipment_zmywarka  \\\n",
      "0                                    brak informacji                   0   \n",
      "1  zmywarka, lodówka, meble, piekarnik, kuchenka,...                   1   \n",
      "2                                    brak informacji                   0   \n",
      "3                                    brak informacji                   0   \n",
      "4             zmywarka, lodówka, piekarnik, kuchenka                   1   \n",
      "5                                              meble                   0   \n",
      "\n",
      "           add_inf  add_inf_piwnica          perks  perks_balkon  \n",
      "0  brak informacji                0  balkon, taras             1  \n",
      "1    pom. użytkowe                0         balkon             1  \n",
      "2    pom. użytkowe                0        ogródek             0  \n",
      "3  brak informacji                0        ogródek             0  \n",
      "4          piwnica                1         balkon             1  \n",
      "5          piwnica                1         balkon             1  \n"
     ]
    }
   ],
   "source": [
    "# Application of the function\n",
    "multiple_choice_transformed_data = multiple_choice_transform(standardized_ownership_labels_data, train_dataset = True)\n",
    "\n",
    "# View the changes that have made to the data\n",
    "vars_before = ['utilities', 'security', 'equipment', 'add_inf', 'perks']\n",
    "vars_after = ['utilities_telewizja kablowa', 'security_domofon / wideofon',\n",
    "              'equipment_zmywarka', 'add_inf_piwnica', 'perks_balkon']\n",
    "\n",
    "multiple_choice_transform_show = pd.concat([data_initial[vars_before].head(6),\n",
    "                                  multiple_choice_transformed_data[vars_after].head(6)],\n",
    "                                 axis=1)\n",
    "\n",
    "print(multiple_choice_transform_show[['utilities', 'utilities_telewizja kablowa',\n",
    "                                     'security', 'security_domofon / wideofon',\n",
    "                                     'equipment', 'equipment_zmywarka',\n",
    "                                     'add_inf', 'add_inf_piwnica',\n",
    "                                     'perks', 'perks_balkon']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### location_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ul. Henryka Strobanda, Wrzosy, Toruń, kujawsko-pomorskie\n",
      "łąkowa 27 B, Stare Polesie, Polesie, Łódź, łódzkie\n",
      "ul. Błękitna, Marki, wołomiński, mazowieckie\n",
      "Szyce, Wielka Wieś, krakowski, małopolskie\n",
      "ul. Rakowiecka 43A, Stary Mokotów, Mokotów, Warszawa, mazowieckie\n"
     ]
    }
   ],
   "source": [
    "# review of the data\n",
    "for i in range(5):\n",
    "    print(data_initial[\"address\"].iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Address structure always looks like this:\n",
    "1. street - one optional field\n",
    "2. city district - two optional field\n",
    "3. city - one obligatory field\n",
    "4. powiat - one optional field\n",
    "5. region (voivodship) - one obligatory field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def address_transform(addressline, cities_dict):\n",
    "    \n",
    "    \"\"\"\n",
    "    Extracts region, location (city/town), and street name from a raw address string.\n",
    "\n",
    "    This function processes a single address line to parse and separate the region,\n",
    "    city (location), and street components based on commas and a dictionary of cities\n",
    "    grouped by regions.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    addressline : str or NaN\n",
    "        A string representing the full address, typically in the format:\n",
    "        'Street, City, Region'. If missing (NaN), the function returns three NaN values.\n",
    "\n",
    "    cities_dict : dict\n",
    "        A dictionary where keys are region names and values are lists of known cities/towns\n",
    "        in that region. Used to validate which element of the address refers to the location.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.Series\n",
    "        A Series containing three values: [region, location, street].\n",
    "    \"\"\"\n",
    "    \n",
    "    # Handle missing or null addresses\n",
    "    if pd.isna(addressline):\n",
    "        return pd.Series([np.nan, np.nan, np.nan])\n",
    "    \n",
    "    # Split the address by commas and strip whitespace\n",
    "    parts = [x.strip() for x in addressline.split(',')]\n",
    "    \n",
    "    # Extract the region (last element)\n",
    "    region = parts[-1]\n",
    "    \n",
    "    # Attempt to identify location (city/town) using cities_dict\n",
    "    city_in_region = cities_dict[region]\n",
    "    \n",
    "    if parts[-2] in city_in_region:\n",
    "        location = parts[-2]\n",
    "    elif len(parts) >= 3 and parts[-3] in city_in_region:\n",
    "        location = parts[-3]\n",
    "    else:\n",
    "        location = np.nan\n",
    "\n",
    "    # Determine if the first part of the address is a valid street\n",
    "    potential_street = parts[0]\n",
    "    if potential_street == location:\n",
    "        street = np.nan\n",
    "    else:\n",
    "        # Remove common prefixes from street names\n",
    "        street = potential_street\n",
    "        for prefix in ['ul. ', 'al. ', 'pl. ']:\n",
    "            street = street.removeprefix(prefix)\n",
    "        street = street.strip()\n",
    "\n",
    "    return pd.Series([region, location, street])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_transform(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Extracts structured geographic information (region, location, and street/district)\n",
    "    from a raw address column using a reference dataset of Polish localities.\n",
    "\n",
    "    This function reads a CSV file containing Polish place names, filters valid settlement types,\n",
    "    and constructs a dictionary that maps regions (voivodeships) to cities and villages.\n",
    "    It then uses this dictionary to parse the 'address' column and extract three elements:\n",
    "    region, location (city/village), and street/district, which are added as new columns.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas.DataFrame\n",
    "        A DataFrame containing a column named 'address' with full address strings.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        The same DataFrame with three new columns:\n",
    "        - 'region': the voivodeship (region) the address belongs to,\n",
    "        - 'location': the specific city/town/village found in the address,\n",
    "        - 'street/district': the remaining part of the address (typically street).\n",
    "    \"\"\"\n",
    "\n",
    "    # Load external CSV file containing Polish place names and administrative regions\n",
    "    locations = pd.read_csv('1. Data Preparation/locations_and_regions.csv')\n",
    "\n",
    "    # Keep only rows with relevant types of settlements\n",
    "    valid_types = ['wieś', 'miasto', 'osada', 'kolonia', 'osada leśna']\n",
    "    locations = locations[locations['Rodzaj'].isin(valid_types)]\n",
    "\n",
    "    # Group places by region (voivodeship)\n",
    "    locations = locations.groupby('Województwo', axis=0)\n",
    "\n",
    "    # Create a dictionary: region -> list of cities/villages in that region\n",
    "    cities_dict = {}\n",
    "    for reg in locations:\n",
    "        cities_dict[reg[0]] = list(reg[1]['Nazwa miejscowości'])\n",
    "\n",
    "    # Manually correct known naming mismatch: Stargard (used to be Stargard Szczeciński)\n",
    "    cities_dict['zachodniopomorskie'].append('Stargard')\n",
    "\n",
    "    # Apply address transformation to extract region, location, and street/district\n",
    "    data[['region', 'location', 'street/district']] = data['address'].apply(\n",
    "        address_transform,\n",
    "        args=[cities_dict]\n",
    "    )\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 46782 entries, 0 to 46781\n",
      "Data columns (total 4 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   address          46782 non-null  object\n",
      " 1   region           46782 non-null  object\n",
      " 2   location         46726 non-null  object\n",
      " 3   street/district  41415 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 1.4+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>region</th>\n",
       "      <th>location</th>\n",
       "      <th>street/district</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ul. Henryka Strobanda, Wrzosy, Toruń, kujawsko...</td>\n",
       "      <td>kujawsko-pomorskie</td>\n",
       "      <td>Toruń</td>\n",
       "      <td>Henryka Strobanda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>łąkowa 27 B, Stare Polesie, Polesie, Łódź, łód...</td>\n",
       "      <td>łódzkie</td>\n",
       "      <td>Łódź</td>\n",
       "      <td>łąkowa 27 B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ul. Błękitna, Marki, wołomiński, mazowieckie</td>\n",
       "      <td>mazowieckie</td>\n",
       "      <td>Marki</td>\n",
       "      <td>Błękitna</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Szyce, Wielka Wieś, krakowski, małopolskie</td>\n",
       "      <td>małopolskie</td>\n",
       "      <td>Wielka Wieś</td>\n",
       "      <td>Szyce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ul. Rakowiecka 43A, Stary Mokotów, Mokotów, Wa...</td>\n",
       "      <td>mazowieckie</td>\n",
       "      <td>Warszawa</td>\n",
       "      <td>Rakowiecka 43A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             address              region  \\\n",
       "0  ul. Henryka Strobanda, Wrzosy, Toruń, kujawsko...  kujawsko-pomorskie   \n",
       "1  łąkowa 27 B, Stare Polesie, Polesie, Łódź, łód...             łódzkie   \n",
       "2       ul. Błękitna, Marki, wołomiński, mazowieckie         mazowieckie   \n",
       "3         Szyce, Wielka Wieś, krakowski, małopolskie         małopolskie   \n",
       "4  ul. Rakowiecka 43A, Stary Mokotów, Mokotów, Wa...         mazowieckie   \n",
       "\n",
       "      location    street/district  \n",
       "0        Toruń  Henryka Strobanda  \n",
       "1         Łódź        łąkowa 27 B  \n",
       "2        Marki           Błękitna  \n",
       "3  Wielka Wieś              Szyce  \n",
       "4     Warszawa     Rakowiecka 43A  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Application of the function\n",
    "location_transformed_data = location_transform(multiple_choice_transformed_data)\n",
    "\n",
    "# View the changes that have made to the data\n",
    "location_transform_var = ['address', 'region', 'location', 'street/district']\n",
    "\n",
    "print(location_transformed_data[location_transform_var].info())\n",
    "location_transformed_data[location_transform_var].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### city_info_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def city_info_transform (data):\n",
    "        \n",
    "    \"\"\"\n",
    "    Enriches the dataset with demographic and administrative information about cities.\n",
    "\n",
    "    This function merges the input dataset with external city-level statistics, such as\n",
    "    population size, population density, and administrative rights (powiat rights).\n",
    "    It also creates categorized bins for population size and density to facilitate analysis.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas.DataFrame\n",
    "        Input DataFrame that must include 'location' and 'region' columns,\n",
    "        typically produced by the `location_transform` function.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        A DataFrame with additional features:\n",
    "        - 'pop_numb_cat': population size category (0-7), where:\n",
    "              0 → up to 10,000 people  \n",
    "              1 → 10,001-20,000  \n",
    "              2 → 20,001-50,000  \n",
    "              3 → 50,001-100,000  \n",
    "              4 → 100,001-250,000  \n",
    "              5 → 250,001-500,000  \n",
    "              6 → 500,001-1,000,000  \n",
    "              7 → more than 1,000,000\n",
    "        - 'pop_dens_cat': population density category (0-7), where:\n",
    "              0 → up to 500 people/km²  \n",
    "              1 → 501-1000  \n",
    "              2 → 1001-1500  \n",
    "              3 → 1501-2000  \n",
    "              4 → 2001-2500  \n",
    "              5 → 2501-3000  \n",
    "              6 → 3001-3500  \n",
    "              7 → more than 3500\n",
    "        - 'with_powiat_rights': binary indicator (0 or 1) whether the city has powiat (county-level) administrative rights.\n",
    "\n",
    "        The function also drops unneeded columns from the merged location data.\n",
    "\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Load city-level demographic and administrative data    \n",
    "    locations_data = pd.read_excel(\"1. Data Preparation/locations_info.xlsx\")\n",
    "    \n",
    "    # Categorize population size into 8 bins\n",
    "    locations_data['pop_numb_cat'] = pd.cut(\n",
    "        locations_data['Liczba ludności'],\n",
    "        bins=[0, 10_000, 20_000, 50_000, 100_000, 250_000, 500_000, 1_000_000, 2_000_000],\n",
    "        labels=np.arange(0, 8)\n",
    "    )\n",
    "    \n",
    "    # Categorize population density into 8 bins (step = 500)\n",
    "    locations_data['pop_dens_cat'] = pd.cut(\n",
    "        locations_data['Gęstość zaludnienia'],\n",
    "        bins=range(1, 4002, 500),\n",
    "        labels=np.arange(0, 8)\n",
    "    )\n",
    "    \n",
    "    # Merge with the main dataset using location and region\n",
    "    data_merged = data.loc[:, 'link':'street/district'].merge(\n",
    "        locations_data,\n",
    "        left_on=['location', 'region'],\n",
    "        right_on=['Miasto', 'Województwo'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Replace NaNs in powiat rights with 0 (default: no rights)    \n",
    "    data_merged['with_powiat_rights'] = data_merged['na_prawach_powiatu'].fillna(0)\n",
    "    \n",
    "    # Convert categories to numeric and fill missing values with 0\n",
    "    # Missing data refers to villages, i.e., the file contains data only for cities\n",
    "    data_merged['pop_numb_cat'] = pd.to_numeric(data_merged['pop_numb_cat']).fillna(0)\n",
    "    data_merged['pop_dens_cat'] = pd.to_numeric(data_merged['pop_dens_cat']).fillna(0)\n",
    "\n",
    "    # Remove unnecessary columns from location metadata\n",
    "    data_merged.drop([\n",
    "        'Miasto', 'Powiat', 'Województwo', 'Powierzchnia',\n",
    "        'Liczba ludności', 'Gęstość zaludnienia', 'na_prawach_powiatu'\n",
    "    ], axis=1, inplace=True)\n",
    "    \n",
    "    return data_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>region</th>\n",
       "      <th>location</th>\n",
       "      <th>street/district</th>\n",
       "      <th>with_powiat_rights</th>\n",
       "      <th>pop_numb_cat</th>\n",
       "      <th>pop_dens_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ul. Henryka Strobanda, Wrzosy, Toruń, kujawsko...</td>\n",
       "      <td>kujawsko-pomorskie</td>\n",
       "      <td>Toruń</td>\n",
       "      <td>Henryka Strobanda</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>łąkowa 27 B, Stare Polesie, Polesie, Łódź, łód...</td>\n",
       "      <td>łódzkie</td>\n",
       "      <td>Łódź</td>\n",
       "      <td>łąkowa 27 B</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ul. Błękitna, Marki, wołomiński, mazowieckie</td>\n",
       "      <td>mazowieckie</td>\n",
       "      <td>Marki</td>\n",
       "      <td>Błękitna</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Szyce, Wielka Wieś, krakowski, małopolskie</td>\n",
       "      <td>małopolskie</td>\n",
       "      <td>Wielka Wieś</td>\n",
       "      <td>Szyce</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ul. Rakowiecka 43A, Stary Mokotów, Mokotów, Wa...</td>\n",
       "      <td>mazowieckie</td>\n",
       "      <td>Warszawa</td>\n",
       "      <td>Rakowiecka 43A</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             address              region  \\\n",
       "0  ul. Henryka Strobanda, Wrzosy, Toruń, kujawsko...  kujawsko-pomorskie   \n",
       "1  łąkowa 27 B, Stare Polesie, Polesie, Łódź, łód...             łódzkie   \n",
       "2       ul. Błękitna, Marki, wołomiński, mazowieckie         mazowieckie   \n",
       "3         Szyce, Wielka Wieś, krakowski, małopolskie         małopolskie   \n",
       "4  ul. Rakowiecka 43A, Stary Mokotów, Mokotów, Wa...         mazowieckie   \n",
       "\n",
       "      location    street/district  with_powiat_rights  pop_numb_cat  \\\n",
       "0        Toruń  Henryka Strobanda                 1.0           4.0   \n",
       "1         Łódź        łąkowa 27 B                 1.0           6.0   \n",
       "2        Marki           Błękitna                 0.0           2.0   \n",
       "3  Wielka Wieś              Szyce                 0.0           0.0   \n",
       "4     Warszawa     Rakowiecka 43A                 1.0           7.0   \n",
       "\n",
       "   pop_dens_cat  \n",
       "0           3.0  \n",
       "1           4.0  \n",
       "2           2.0  \n",
       "3           0.0  \n",
       "4           6.0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Application of the function\n",
    "city_info_transformed_data = city_info_transform(location_transformed_data)\n",
    "\n",
    "# View the changes that have made to the data\n",
    "location_transform_var = ['address', 'region','location','street/district',\n",
    "                          'with_powiat_rights', 'pop_numb_cat',\n",
    "                          'pop_dens_cat']\n",
    "city_info_transformed_data[location_transform_var].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preliminary_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preliminary_transform (data, train_dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    Performs a full preliminary transformation pipeline on the input dataset.\n",
    "\n",
    "    This function executes a sequence of data cleaning and feature engineering steps\n",
    "    that prepare the dataset for further analysis or modeling. It ensures consistency\n",
    "    in missing values, encodes categorical data, standardizes formats, and enriches\n",
    "    the data with external geographic and demographic information.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas.DataFrame\n",
    "        The raw dataset to be processed. Must contain all necessary columns \n",
    "        such as 'floor', 'year', 'address', etc.\n",
    "\n",
    "    train_dataset : bool\n",
    "        Indicates whether the data being processed is training data.\n",
    "        This is used to display warnings about unexpected new categories\n",
    "        in multi-label (multi-choice) variables.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        A cleaned and enriched DataFrame ready for modeling or further processing.\n",
    "\n",
    "    The pipeline performs the following transformations:\n",
    "    ----------------------------------------------------\n",
    "    1. 'standardize_missing_values' - Converts placeholders like \"Zapytaj o cenę\" to NaN.\n",
    "    2. 'clean_numeric_columns' - Cleans and converts numeric columns such as 'price', 'area', 'rent'.\n",
    "    3. 'categorize_rent' - Categorizes rental prices into bins.\n",
    "    4. 'process_floor_data' - Splits and normalizes apartment floor info.\n",
    "    5. 'fill_missing_categoricals' - Fills missing categorical values with \"nie podano\".\n",
    "    6. 'encode_parking_presence' - Encodes binary presence of parking (e.g., yes/no).\n",
    "    7. 'convert_year_to_int' - Converts the 'year' column is of integer type.\n",
    "    8. 'standardize_ownership_labels' - Standardizes ownership labels (e.g., unifying similar terms).\n",
    "    9. 'multiple_choice_transform' - One-hot encodes multi-choice variables based on a predefined dictionary.\n",
    "    10. 'location_transform' - Extracts region, city, and street from the address.\n",
    "    11. 'city_info_transform' - Adds population and administrative info by merging with external city data.\n",
    "    \"\"\"\n",
    "    \n",
    "    standardized_missing_values_data = standardize_missing_values(data)\n",
    "    cleaned_numeric_columns_data = clean_numeric_columns(standardized_missing_values_data)\n",
    "    categorized_rent_data = categorize_rent(cleaned_numeric_columns_data)\n",
    "    processed_floor_data = process_floor_data(categorized_rent_data)\n",
    "    fill_missing_categoricals_data = fill_missing_categoricals(processed_floor_data)\n",
    "    encoded_parking_presence_data = encode_parking_presence(fill_missing_categoricals_data)\n",
    "    converted_year_to_int_data = convert_year_to_int(encoded_parking_presence_data)\n",
    "    standardized_ownership_labels_data = standardize_ownership_labels(converted_year_to_int_data)\n",
    "    multiple_choice_transformed_data = multiple_choice_transform(standardized_ownership_labels_data,\n",
    "                                                                 train_dataset)\n",
    "    location_transformed_data = location_transform(multiple_choice_transformed_data)\n",
    "    city_info_transformed_data = city_info_transform(location_transformed_data)\n",
    "    \n",
    "    return city_info_transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the categorizations occurring in the set in multi-vector selection variables were coded.\n"
     ]
    }
   ],
   "source": [
    "# Application of the function\n",
    "preliminary_transformed_data = preliminary_transform(data_initial, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 46782 entries, 0 to 46781\n",
      "Data columns (total 51 columns):\n",
      " #   Column                                Non-Null Count  Dtype  \n",
      "---  ------                                --------------  -----  \n",
      " 0   link                                  46782 non-null  object \n",
      " 1   price                                 44673 non-null  float64\n",
      " 2   address                               46782 non-null  object \n",
      " 3   area                                  46782 non-null  float64\n",
      " 4   num_rooms                             46782 non-null  int64  \n",
      " 5   ownership_status                      46782 non-null  object \n",
      " 6   flat_condition                        46782 non-null  object \n",
      " 7   parking                               26253 non-null  object \n",
      " 8   heating                               46782 non-null  object \n",
      " 9   market                                46782 non-null  object \n",
      " 10  ad_type                               46782 non-null  object \n",
      " 11  availability                          8495 non-null   object \n",
      " 12  year                                  46756 non-null  Int64  \n",
      " 13  devel_type                            46773 non-null  object \n",
      " 14  windows                               46782 non-null  object \n",
      " 15  lift                                  46782 non-null  object \n",
      " 16  mater                                 46782 non-null  object \n",
      " 17  rent_cat                              21268 non-null  float64\n",
      " 18  number_floor_in_building              44544 non-null  float64\n",
      " 19  ap_floor                              45547 non-null  float64\n",
      " 20  parking_coded                         46782 non-null  int64  \n",
      " 21  utilities_telewizja kablowa           46782 non-null  int64  \n",
      " 22  utilities_internet                    46782 non-null  int64  \n",
      " 23  utilities_telefon                     46782 non-null  int64  \n",
      " 24  security_drzwi / okna antywłamaniowe  46782 non-null  int64  \n",
      " 25  security_teren zamknięty              46782 non-null  int64  \n",
      " 26  security_domofon / wideofon           46782 non-null  int64  \n",
      " 27  security_monitoring / ochrona         46782 non-null  int64  \n",
      " 28  security_rolety antywłamaniowe        46782 non-null  int64  \n",
      " 29  security_system alarmowy              46782 non-null  int64  \n",
      " 30  equipment_zmywarka                    46782 non-null  int64  \n",
      " 31  equipment_lodówka                     46782 non-null  int64  \n",
      " 32  equipment_meble                       46782 non-null  int64  \n",
      " 33  equipment_piekarnik                   46782 non-null  int64  \n",
      " 34  equipment_kuchenka                    46782 non-null  int64  \n",
      " 35  equipment_pralka                      46782 non-null  int64  \n",
      " 36  equipment_telewizor                   46782 non-null  int64  \n",
      " 37  add_inf_pom. użytkowe                 46782 non-null  int64  \n",
      " 38  add_inf_piwnica                       46782 non-null  int64  \n",
      " 39  add_inf_dwupoziomowe                  46782 non-null  int64  \n",
      " 40  add_inf_oddzielna kuchnia             46782 non-null  int64  \n",
      " 41  add_inf_klimatyzacja                  46782 non-null  int64  \n",
      " 42  perks_balkon                          46782 non-null  int64  \n",
      " 43  perks_taras                           46782 non-null  int64  \n",
      " 44  perks_ogródek                         46782 non-null  int64  \n",
      " 45  region                                46782 non-null  object \n",
      " 46  location                              46726 non-null  object \n",
      " 47  street/district                       41415 non-null  object \n",
      " 48  pop_numb_cat                          46782 non-null  float64\n",
      " 49  pop_dens_cat                          46782 non-null  float64\n",
      " 50  with_powiat_rights                    46782 non-null  float64\n",
      "dtypes: Int64(1), float64(8), int64(26), object(16)\n",
      "memory usage: 18.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Information for changed data\n",
    "preliminary_transformed_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing and outlier observations\n",
    "Analysis of missing and outlier observations was conducted in the file: **missvalue_outliers_analysis** <br>\n",
    "The file shows the distributions of each variable, establishes the variables taken into the model, and sets limits beyond which an observation will be considered an outlier. <br><br>\n",
    "\n",
    "In summary, the model will be trained on the data of apartments which:<br>\n",
    "1. price is in the range of 100 thousand PLN to 1,5 million PLN\n",
    "2. area is in the range of 20 m² to 200 m²\n",
    "3. is in a building that has less than 20 floors\n",
    "4. is in a building that was built no earlier than 1900\n",
    "5. have no missing data in the variables 'area', 'num_rooms', 'market', 'devel_type', 'ad_type', 'location', 'region'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, in some categorical variables, categories that occurred less frequently than about 5% of the set were converted to “other” (\"inny\" in Polish) categories. These were the variables:\n",
    "1. **ownership_status** - 'spółdzielcze wł. prawo do lokalu', 'udział', 'użytkowanie wieczyste / dzierżawa'\n",
    "2. **heating** - 'kotłownia', 'elektryczne', 'piece kaflowe'\n",
    "3. **devel_type** - 'plomba', 'loft', 'dom wolnostojący', 'szeregowiec'\n",
    "4. **windows** - 'drewniane', 'aluminiowe'\n",
    "5. **mater** - 'drewno', 'keramzyt', 'beton', 'beton komórkowy', 'żelbet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dictionary with the defined range of outlier values\n",
    "outlier_values_dict = {\"max_floor\": 20,\n",
    "                       \"min_price\": 100_000,\n",
    "                       \"max_price\": 1_500_000,\n",
    "                       \"min_area\": 20,\n",
    "                       \"max_area\": 200,\n",
    "                       \"min_year\": 1900,\n",
    "                       \"categories_to_replace\": ['spółdzielcze wł. prawo do lokalu', 'udział', 'użytkowanie wieczyste / dzierżawa',\n",
    "                                                 'kotłownia', 'elektryczne', 'piece kaflowe',\n",
    "                                                 'plomba', 'loft', 'dom wolnostojący', 'szeregowiec',\n",
    "                                                 'drewniane', 'aluminiowe',\n",
    "                                                 'drewno', 'keramzyt', 'beton', 'beton komórkowy', 'żelbet',\n",
    "                                                 'inne', 'inny'],\n",
    "                       \"variables_to_drop_na\": ['area', 'num_rooms', 'market', 'devel_type', 'ad_type', 'location', 'region']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. Data Preparation/outlier_values_dict.joblib']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving the dictionary\n",
    "outlier_values_dict_path = \"1. Data Preparation/outlier_values_dict.joblib\"\n",
    "joblib.dump(outlier_values_dict, outlier_values_dict_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_missing_values(data, train_dataset, variables_to_drop_na):\n",
    "\n",
    "    \"\"\"\n",
    "    Replaces specified outlier or rare category values in all columns of the dataset with a general label 'inny' (Polish for \"other\").\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        The input dataset containing categorical variables.\n",
    "\n",
    "    categories_to_replace : list of str\n",
    "        A list of category values (strings) that should be considered outliers or rare,\n",
    "        and replaced with the label 'inny' (Polish for \"other\").\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    data_clean : pd.DataFrame\n",
    "        A cleaned version of the input dataset with specified category values replaced by 'inny'.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create copy of the original dataset to avoid modifying it in-place\n",
    "    data_clean_missing = data.copy()\n",
    "    \n",
    "    # Remove rows with NA values in the specified columns\n",
    "    data_clean_missing = data_clean_missing.dropna(subset = variables_to_drop_na)\n",
    "    \n",
    "    # If this is test data, print a message about removed rows due to missing values\n",
    "    if not train_dataset:\n",
    "        if len(data) > len(data_clean_missing):\n",
    "            print(\"\"\"\n",
    "                  W zbiorze wystąpiły braki w cechach: 'typ budynku', 'powierzchnia',\n",
    "                  'liczba pokoi', 'rynek', 'ogłoszeniodawca', 'miasto', 'województwo'.\n",
    "                  Do otrzymania prognozy wszystkie z powyższych cech\n",
    "                  muszą być wypełnione.\n",
    "                  Obserwacje te zostały usunięte ze zbioru do predykcji.\n",
    "                  \"\"\")    \n",
    "    \n",
    "    return data_clean_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### proceed_outlier_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proceed_outlier_categories(data, categories_to_replace):\n",
    "    \n",
    "    \"\"\"\n",
    "    Replaces outlier category values in the dataset with a general label 'inny'.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        The input dataset containing categorical variables.\n",
    "    \n",
    "    categories_to_replace : list\n",
    "        A list of categories values\n",
    "        that should be replaced with the label 'inny' (Polish for \"other\").\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    data_clean : pd.DataFrame\n",
    "        A cleaned version of the input dataset with specified category values replaced.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create copy of the original dataset to avoid modifying it in-place\n",
    "    data_clean = data.copy()\n",
    "    \n",
    "    # Replace rare or inconsistent category values with 'inny' (Polish for \"other\")\n",
    "    data_clean = data_clean.replace(categories_to_replace, 'inny')\n",
    "    \n",
    "    return data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### replace_mistakes_with_na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_mistakes_with_na(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    Cleans data by replacing obviously incorrect or implausible values in selected numerical columns with NaN.\n",
    "\n",
    "    Specifically:\n",
    "    - Replaces values above 100 in 'number_floor_in_building' and 'ap_floor' with NaN,\n",
    "      as these are likely due to data entry errors.\n",
    "    - Replaces values in the 'year' column below 1700 with NaN, assuming such years are invalid\n",
    "      for apartment construction.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        The input dataset to be cleaned.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    data_clean : pd.DataFrame\n",
    "        The cleaned dataset with incorrect values replaced by NaN.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create copy of the original dataset to avoid modifying it in-place\n",
    "    data_clean = data.copy()\n",
    "    \n",
    "    # Set floor information with floor number above 100 to NaN\n",
    "    data_clean['number_floor_in_building'] = (\n",
    "        data_clean['number_floor_in_building']\n",
    "        .apply(lambda x: x if pd.isna(x) or x <= 100 else np.nan)\n",
    "    )\n",
    "    data_clean['number_floor_in_building'] = pd.to_numeric(data_clean['number_floor_in_building'])\n",
    "\n",
    "    data_clean['ap_floor'] = (\n",
    "        data_clean['ap_floor']\n",
    "        .apply(lambda x: x if pd.isna(x) or x <= 100 else np.nan)\n",
    "    )\n",
    "    data_clean['ap_floor'] = pd.to_numeric(data_clean['ap_floor'])\n",
    "\n",
    "    # Set construction years below 1700 to NaN\n",
    "    data_clean['year'] = (\n",
    "        data_clean['year']\n",
    "        .apply(lambda x: x if pd.isna(x) or x >= 1700 else np.nan)\n",
    "    )\n",
    "    data_clean['year'] = pd.to_numeric(data_clean['year']).astype('Int64')\n",
    "    \n",
    "    return data_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### proceed_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proceed_outliers(data, train_dataset,\n",
    "                     minprice, maxprice,\n",
    "                     minarea, maxarea,\n",
    "                     maxfloor, minyear):\n",
    "    \n",
    "    \"\"\"\n",
    "    Filters out observations that are likely outliers based on provided thresholds \n",
    "    for selected numerical features such as price, area, floor level, and year of construction.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        The input dataset containing apartment listings.\n",
    "\n",
    "    train_dataset : bool\n",
    "        Indicates whether the data is used for training (True) or inference/prediction (False).\n",
    "\n",
    "    minprice : float\n",
    "        Minimum acceptable price. Listings with a lower price are considered outliers.\n",
    "\n",
    "    maxprice : float\n",
    "        Maximum acceptable price. Listings with a higher price are considered outliers.\n",
    "\n",
    "    minarea : float\n",
    "        Minimum acceptable area (m²). Listings with a smaller area are considered outliers.\n",
    "\n",
    "    maxarea : float\n",
    "        Maximum acceptable area (m²). Listings with a larger area are considered outliers.\n",
    "\n",
    "    maxfloor : int\n",
    "        Maximum acceptable floor level. Higher floor numbers are treated as outliers.\n",
    "\n",
    "    minyear : int\n",
    "        Minimum acceptable construction year. Years below this threshold are treated as invalid.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Cleaned dataset with extreme values filtered out (if `train_dataset=True`) \n",
    "        or the original dataset with a warning printed about outliers (if `train_dataset=False`).\n",
    "    \"\"\"\n",
    "\n",
    "    # Create copy of the original dataset to avoid modifying it in-place\n",
    "    data_clean = data.copy()\n",
    "    \n",
    "    # Filter observations based on price range\n",
    "    data_clean = data_clean[data_clean['price'] <= maxprice]\n",
    "    data_clean = data_clean[data_clean['price'] >= minprice]\n",
    "    \n",
    "    # Filter observations based on area range\n",
    "    data_clean = data_clean[\n",
    "        (data_clean['area'] <= maxarea) |\n",
    "        (data_clean['area'].isna())\n",
    "    ]\n",
    "    data_clean = data_clean[\n",
    "        (data_clean['area'] >= minarea)|\n",
    "        (data_clean['area'].isna())\n",
    "    ]\n",
    "    \n",
    "    # Filter based on floor information (if not missing)\n",
    "    data_clean = data_clean[\n",
    "        (data_clean['number_floor_in_building'] <= maxfloor) | \n",
    "        (data_clean['number_floor_in_building'].isna())\n",
    "    ]\n",
    "    data_clean = data_clean[\n",
    "        (data_clean['ap_floor'] <= maxfloor) |\n",
    "        (data_clean['ap_floor'].isna())\n",
    "    ]\n",
    "\n",
    "    # Filter based on year of building information\n",
    "    data_clean = data_clean[\n",
    "        (data_clean['year'] >= minyear) |\n",
    "        (data_clean['year'].isna())\n",
    "    ]\n",
    "    \n",
    "    if train_dataset:\n",
    "        # For training data, return only the cleaned records\n",
    "        return data_clean\n",
    "    else:\n",
    "        # For inference data, return full data but warn about outliers\n",
    "        if len(data) > (len(data_clean) + sum(data['price'].isna())):\n",
    "            print(\"\"\"\n",
    "                  W zbiorze wystąpiły obserwację o skrajnych wartościach\n",
    "                  ze względu na zaproponowaną cenę, powierzchnię, piętro lub rok budynku.\n",
    "                  Może zmniejszeć dokładność prognozy.\n",
    "                  \"\"\")\n",
    "            \n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cleaning_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning_data (data, train_dataset):\n",
    "    \n",
    "    \"\"\"\n",
    "    Applies a sequence of data cleaning operations on a dataset, including:\n",
    "    - Dropping observations with missing values in key variables\n",
    "    - Replacing rare categorical values\n",
    "    - Replacing clearly incorrect numerical values (e.g., year < 1700, floors > 100) with missing values\n",
    "    - Removing outliers based on defined thresholds\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        The input dataset to be cleaned.\n",
    "\n",
    "    train_dataset : bool\n",
    "        Indicates whether the data is used for model training (True)\n",
    "        or for inference/prediction (False). This affects how aggressively outliers and missing data are removed.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Cleaned dataset after all preprocessing steps.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load thresholds and rules for outlier and missing value handling from file\n",
    "    outlier_values_dict = joblib.load(\"1. Data Preparation/outlier_values_dict.joblib\")\n",
    "    \n",
    "    variables_to_drop_na = outlier_values_dict[\"variables_to_drop_na\"]\n",
    "    categories_to_replace = outlier_values_dict[\"categories_to_replace\"]\n",
    "    minprice = outlier_values_dict[\"min_price\"]\n",
    "    maxprice = outlier_values_dict[\"max_price\"]\n",
    "    minarea = outlier_values_dict[\"min_area\"]\n",
    "    maxarea = outlier_values_dict[\"max_area\"]\n",
    "    maxfloor = outlier_values_dict[\"max_floor\"]\n",
    "    minyear = outlier_values_dict[\"min_year\"]\n",
    "    \n",
    "    # Drop observations with missing values in critical variables\n",
    "    cleaned_missind_values_data = clean_missing_values(data, train_dataset, variables_to_drop_na)\n",
    "    \n",
    "    # Replace rare categorical values with a standard label\n",
    "    proceeded_outlier_cat_data = proceed_outlier_categories(cleaned_missind_values_data, categories_to_replace)\n",
    "    \n",
    "    # Replace implausible or erroneous numeric values with NaN\n",
    "    replaced_mistakes_data = replace_mistakes_with_na(proceeded_outlier_cat_data)\n",
    "    \n",
    "    # Remove or flag outliers based on provided thresholds\n",
    "    procceded_outliers_data = proceed_outliers(replaced_mistakes_data,\n",
    "                                               train_dataset,\n",
    "                                               minprice, maxprice,\n",
    "                                               minarea, maxarea,\n",
    "                                               maxfloor, minyear)\n",
    "    \n",
    "    return procceded_outliers_data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations before cleaning of outliers: 46782\n",
      "Number of observations after cleaning of outliers: 41862\n",
      "Data set decreased by 4920 ads (10.52%)\n"
     ]
    }
   ],
   "source": [
    "# Application of the function\n",
    "cleaned_outliers_data = cleaning_data(preliminary_transformed_data, True)\n",
    "\n",
    "# View the changes that have made to the data\n",
    "len_before = len(preliminary_transformed_data)\n",
    "len_after = len(cleaned_outliers_data)\n",
    "\n",
    "print(f\"Number of observations before cleaning of outliers: {len_before}\")\n",
    "print(f\"Number of observations after cleaning of outliers: {len_after}\")\n",
    "\n",
    "len_change = np.round((1 - len_after/len_before)*100, 2)\n",
    "print(f\"Data set decreased by {len_before-len_after} ads ({len_change}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining variables for the model\n",
    "features_to_use = ['area', 'num_rooms', 'ownership_status', 'flat_condition',\n",
    "                   'parking_coded', 'heating', 'ad_type', 'year', 'devel_type',\n",
    "                   'windows','lift', 'mater', 'rent_cat', 'market',\n",
    "                   'number_floor_in_building', 'ap_floor',\n",
    "                   \n",
    "                   'utilities_telewizja kablowa', 'utilities_internet', 'utilities_telefon',\n",
    "                   \n",
    "                   'security_drzwi / okna antywłamaniowe','security_teren zamknięty',\n",
    "                   'security_domofon / wideofon', 'security_monitoring / ochrona',\n",
    "                   'security_rolety antywłamaniowe', 'security_system alarmowy',\n",
    "                   \n",
    "                   'equipment_zmywarka', 'equipment_lodówka', 'equipment_meble',\n",
    "                   'equipment_piekarnik', 'equipment_kuchenka', 'equipment_pralka', 'equipment_telewizor',\n",
    "                   \n",
    "                   'add_inf_pom. użytkowe', 'add_inf_piwnica', 'add_inf_dwupoziomowe',\n",
    "                   'add_inf_oddzielna kuchnia', 'add_inf_klimatyzacja',\n",
    "                   \n",
    "                   'perks_balkon', 'perks_taras', 'perks_ogródek',\n",
    "                   \n",
    "                   'region', 'with_powiat_rights', 'pop_numb_cat', 'pop_dens_cat']\n",
    "\n",
    "target = 'price'\n",
    "\n",
    "features_to_onehotencode = ['ownership_status', 'flat_condition', 'heating', 'market',\n",
    "                            'ad_type', 'windows', 'lift', 'mater', 'devel_type', 'region']\n",
    "\n",
    "features_for_kNN_impute = ['rent_cat','year','ap_floor','number_floor_in_building',\n",
    "                           'devel_type', 'lift', 'market',\n",
    "                           'perks_balkon', 'perks_taras', 'perks_ogródek']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. Data Preparation/model_features_dict.joblib']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creation of a dictionary of variables\n",
    "model_features_dict = {\"features_to_use\": features_to_use,\n",
    "                       \"target\": target,\n",
    "                       \"features_to_onehotencode\": features_to_onehotencode,\n",
    "                       \"features_for_kNN_impute\": features_for_kNN_impute}\n",
    "\n",
    "# saving the dictionary\n",
    "model_features_dict_path = \"1. Data Preparation/model_features_dict.joblib\"\n",
    "joblib.dump(model_features_dict, model_features_dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if there are all variables in the set for analysisdefine variables for the model\n",
    "model_features_dict = joblib.load(\"1. Data Preparation/model_features_dict.joblib\")\n",
    "    \n",
    "all(x in cleaned_outliers_data.columns for x in model_features_dict['features_to_use'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing for model selection\n",
    "Before modeling, the data were processed as follows:\n",
    "1. missing and outlier data was cleaned - **cleaning_data**\n",
    "2. categorical features were one-hot encoded - **one_hot_encode_train_test**\n",
    "3. all features were scale with z-score normalization - **scale_train_test**\n",
    "4. missing data was filled usin kNN imputation - **kNN_impute_train_test**\n",
    "5. data was splitted into train and test datasets and all the steps above were combined - **prepare_train_test_data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                  W zbiorze wystąpiły braki w cechach: 'typ budynku', 'powierzchnia',\n",
      "                  'liczba pokoi', 'rynek', 'ogłoszeniodawca', 'miasto', 'województwo'.\n",
      "                  Do otrzymania prognozy wszystkie z powyższych cech\n",
      "                  muszą być wypełnione.\n",
      "                  Obserwacje te zostały usunięte ze zbioru do predykcji.\n",
      "                  \n",
      "\n",
      "                  W zbiorze wystąpiły obserwację o skrajnych wartościach\n",
      "                  ze względu na zaproponowaną cenę, powierzchnię, piętro lub rok budynku.\n",
      "                  Może zmniejszeć dokładność prognozy.\n",
      "                  \n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "train_data, test_data= train_test_split(preliminary_transformed_data,\n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state = 99)\n",
    "\n",
    "# Clean the train and test data\n",
    "train_cleaned_data = cleaning_data(train_data, train_dataset = True)\n",
    "test_cleaned_data = cleaning_data(test_data, train_dataset = False)\n",
    "\n",
    "# Extract features (X) and target variable (y)\n",
    "X_train = train_cleaned_data[features_to_use]\n",
    "y_train = train_cleaned_data[target]\n",
    "\n",
    "X_test = test_cleaned_data[features_to_use]\n",
    "y_test = test_cleaned_data[target]\n",
    "\n",
    "# Ensure NaNs are np.nan, not pd.NA\n",
    "X_train = X_train.replace({pd.NA: np.nan})\n",
    "X_test = X_test.replace({pd.NA: np.nan})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one_hot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_train_test(X_train_to_encode, X_test_to_encode, features_to_onehotencode):\n",
    "    \n",
    "    \"\"\"\n",
    "    One-hot encodes selected categorical features with category dropping for stability.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train_to_encode : pd.DataFrame\n",
    "        Training data.\n",
    "    X_test_to_encode : pd.DataFrame\n",
    "        Test data.\n",
    "    features_to_onehotencode : list of str\n",
    "        List of categorical feature names to be one-hot encoded.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X_train : pd.DataFrame\n",
    "        Training data with encoded features.\n",
    "    X_test : pd.DataFrame\n",
    "        Test data with encoded features.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create copies of the original datasets to avoid modifying them in-place\n",
    "    X_train = X_train_to_encode.copy()\n",
    "    X_test = X_test_to_encode.copy()\n",
    "    \n",
    "    for column in features_to_onehotencode:\n",
    "        # Define categories to drop if found\n",
    "        categories_to_drop = ['nie podano', 'inny', 'wtórny', 'prywatny', 'opolskie']\n",
    "        \n",
    "        # Decide which category to drop fot this feature\n",
    "        cat_to_drop = None\n",
    "        for cat in categories_to_drop:\n",
    "            if cat in X_train[column].unique():\n",
    "                cat_to_drop = [cat]\n",
    "                break\n",
    "                \n",
    "        # Create and fit the encoder\n",
    "        encoder = OneHotEncoder(drop = cat_to_drop if cat_to_drop else 'first', sparse=False, handle_unknown='ignore')\n",
    "        encoder.fit(X_train[[column]])\n",
    "        \n",
    "        # Column names for output\n",
    "        cols = encoder.get_feature_names_out([column])\n",
    "        \n",
    "        # Transform both train and test\n",
    "        X_train_encoded = pd.DataFrame(encoder.transform(X_train[[column]]), columns=cols, index=X_train.index)\n",
    "        X_test_encoded = pd.DataFrame(encoder.transform(X_test[[column]]), columns=cols, index=X_test.index)\n",
    "        \n",
    "        # Join with original data\n",
    "        X_train = pd.concat([X_train, X_train_encoded], axis=1)\n",
    "        X_test = pd.concat([X_test, X_test_encoded], axis=1)\n",
    "        \n",
    "    # Drop the original categorical column\n",
    "    X_train.drop(features_to_onehotencode, axis=1, inplace=True)\n",
    "    X_test.drop(features_to_onehotencode, axis=1, inplace=True)\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 33509 entries, 4068 to 29313\n",
      "Data columns (total 37 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   ownership_status_inny            33509 non-null  float64\n",
      " 1   ownership_status_pełna własność  33509 non-null  float64\n",
      " 2   flat_condition_do remontu        33509 non-null  float64\n",
      " 3   flat_condition_do wykończenia    33509 non-null  float64\n",
      " 4   flat_condition_do zamieszkania   33509 non-null  float64\n",
      " 5   heating_gazowe                   33509 non-null  float64\n",
      " 6   heating_inny                     33509 non-null  float64\n",
      " 7   heating_miejskie                 33509 non-null  float64\n",
      " 8   market_pierwotny                 33509 non-null  float64\n",
      " 9   ad_type_biuro nieruchomości      33509 non-null  float64\n",
      " 10  ad_type_deweloper                33509 non-null  float64\n",
      " 11  windows_inny                     33509 non-null  float64\n",
      " 12  windows_plastikowe               33509 non-null  float64\n",
      " 13  lift_tak                         33509 non-null  float64\n",
      " 14  mater_cegła                      33509 non-null  float64\n",
      " 15  mater_inny                       33509 non-null  float64\n",
      " 16  mater_pustak                     33509 non-null  float64\n",
      " 17  mater_silikat                    33509 non-null  float64\n",
      " 18  mater_wielka płyta               33509 non-null  float64\n",
      " 19  devel_type_apartamentowiec       33509 non-null  float64\n",
      " 20  devel_type_blok                  33509 non-null  float64\n",
      " 21  devel_type_kamienica             33509 non-null  float64\n",
      " 22  region_dolnośląskie              33509 non-null  float64\n",
      " 23  region_kujawsko-pomorskie        33509 non-null  float64\n",
      " 24  region_lubelskie                 33509 non-null  float64\n",
      " 25  region_lubuskie                  33509 non-null  float64\n",
      " 26  region_mazowieckie               33509 non-null  float64\n",
      " 27  region_małopolskie               33509 non-null  float64\n",
      " 28  region_podkarpackie              33509 non-null  float64\n",
      " 29  region_podlaskie                 33509 non-null  float64\n",
      " 30  region_pomorskie                 33509 non-null  float64\n",
      " 31  region_warmińsko-mazurskie       33509 non-null  float64\n",
      " 32  region_wielkopolskie             33509 non-null  float64\n",
      " 33  region_zachodniopomorskie        33509 non-null  float64\n",
      " 34  region_łódzkie                   33509 non-null  float64\n",
      " 35  region_śląskie                   33509 non-null  float64\n",
      " 36  region_świętokrzyskie            33509 non-null  float64\n",
      "dtypes: float64(37)\n",
      "memory usage: 9.7 MB\n",
      "None\n",
      "       ownership_status_inny  ownership_status_pełna własność  \\\n",
      "4068                     0.0                              1.0   \n",
      "21202                    0.0                              0.0   \n",
      "44497                    0.0                              1.0   \n",
      "22440                    0.0                              1.0   \n",
      "16880                    0.0                              1.0   \n",
      "\n",
      "       flat_condition_do remontu  flat_condition_do wykończenia  \\\n",
      "4068                         0.0                            0.0   \n",
      "21202                        0.0                            0.0   \n",
      "44497                        0.0                            1.0   \n",
      "22440                        0.0                            0.0   \n",
      "16880                        0.0                            1.0   \n",
      "\n",
      "       flat_condition_do zamieszkania  heating_gazowe  heating_inny  \\\n",
      "4068                              0.0             0.0           0.0   \n",
      "21202                             0.0             0.0           0.0   \n",
      "44497                             0.0             0.0           0.0   \n",
      "22440                             0.0             0.0           0.0   \n",
      "16880                             0.0             0.0           0.0   \n",
      "\n",
      "       heating_miejskie  market_pierwotny  ad_type_biuro nieruchomości  ...  \\\n",
      "4068                0.0               1.0                          1.0  ...   \n",
      "21202               0.0               1.0                          1.0  ...   \n",
      "44497               0.0               1.0                          1.0  ...   \n",
      "22440               1.0               0.0                          1.0  ...   \n",
      "16880               1.0               1.0                          1.0  ...   \n",
      "\n",
      "       region_małopolskie  region_podkarpackie  region_podlaskie  \\\n",
      "4068                  0.0                  0.0               0.0   \n",
      "21202                 1.0                  0.0               0.0   \n",
      "44497                 0.0                  0.0               0.0   \n",
      "22440                 0.0                  0.0               0.0   \n",
      "16880                 1.0                  0.0               0.0   \n",
      "\n",
      "       region_pomorskie  region_warmińsko-mazurskie  region_wielkopolskie  \\\n",
      "4068                0.0                         0.0                   0.0   \n",
      "21202               0.0                         0.0                   0.0   \n",
      "44497               0.0                         0.0                   0.0   \n",
      "22440               0.0                         0.0                   0.0   \n",
      "16880               0.0                         0.0                   0.0   \n",
      "\n",
      "       region_zachodniopomorskie  region_łódzkie  region_śląskie  \\\n",
      "4068                         0.0             0.0             0.0   \n",
      "21202                        0.0             0.0             0.0   \n",
      "44497                        0.0             0.0             1.0   \n",
      "22440                        0.0             0.0             0.0   \n",
      "16880                        0.0             0.0             0.0   \n",
      "\n",
      "       region_świętokrzyskie  \n",
      "4068                     0.0  \n",
      "21202                    0.0  \n",
      "44497                    0.0  \n",
      "22440                    0.0  \n",
      "16880                    0.0  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "# Application of the function\n",
    "X_train_encoded, X_test_encoded = one_hot_encode_train_test(X_train, X_test, features_to_onehotencode)\n",
    "\n",
    "# View the changes that have made to the data\n",
    "vars_after_encoding = []\n",
    "for var in features_to_onehotencode:\n",
    "        col_after_encode = [col for col in X_train_encoded.columns if col.startswith(var)]\n",
    "        vars_after_encoding = vars_after_encoding + col_after_encode\n",
    "        \n",
    "print(X_train_encoded[vars_after_encoding].info())\n",
    "print(X_train_encoded[vars_after_encoding].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_train_test(X_train_to_scale, X_test_to_scale):\n",
    "    \n",
    "    \"\"\"\n",
    "    Standardizes numerical features in training and test datasets using StandardScaler.\n",
    "\n",
    "    This function applies z-score normalization to ensure each feature has a mean of 0\n",
    "    and standard deviation of 1. It fits the scaler only on the training data and uses\n",
    "    the same transformation on the test data to avoid data leakage.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train_to_scale : pd.DataFrame\n",
    "        The training dataset with numerical features to be standardized.\n",
    "\n",
    "    X_test_to_scale : pd.DataFrame\n",
    "        The test dataset with the same structure as X_train_to_scale.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X_train : pd.DataFrame\n",
    "        The standardized training dataset.\n",
    "\n",
    "    X_test : pd.DataFrame\n",
    "        The standardized test dataset, transformed using the scaler fitted on training data.\n",
    "    \"\"\"\n",
    "    # Create copies of the original datasets to avoid modifying them in-place\n",
    "    X_train = X_train_to_scale.copy()\n",
    "    X_test = X_test_to_scale.copy() \n",
    "    \n",
    "    # Create and fit the scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train = pd.DataFrame(scaler.fit_transform(X_train), columns = X_train.columns)\n",
    "    X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n",
    "        \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       area  num_rooms  parking_coded      year  rent_cat  \\\n",
      "0 -0.148860   0.403396       0.919684  0.728266       NaN   \n",
      "1  1.920864   0.403396       0.919684  0.696106       NaN   \n",
      "2  1.040200   0.403396      -1.087330  0.696106 -0.960480   \n",
      "3 -0.488189  -0.714575      -1.087330 -0.365186  0.773109   \n",
      "4 -0.811584  -0.714575       0.919684  0.760427 -0.960480   \n",
      "\n",
      "   number_floor_in_building  ap_floor  utilities_telewizja kablowa  \\\n",
      "0                       NaN       NaN                    -0.839817   \n",
      "1                 -0.064076  0.516527                     1.190736   \n",
      "2                 -1.212746 -0.543905                    -0.839817   \n",
      "3                  1.084593 -0.543905                     1.190736   \n",
      "4                 -0.064076 -0.013689                     1.190736   \n",
      "\n",
      "   utilities_internet  utilities_telefon  ...  region_małopolskie  \\\n",
      "0           -1.023096          -0.564978  ...           -0.307281   \n",
      "1            0.977425           1.769981  ...            3.254351   \n",
      "2           -1.023096          -0.564978  ...           -0.307281   \n",
      "3            0.977425          -0.564978  ...           -0.307281   \n",
      "4            0.977425           1.769981  ...            3.254351   \n",
      "\n",
      "   region_podkarpackie  region_podlaskie  region_pomorskie  \\\n",
      "0             -0.18724          -0.14338         -0.364716   \n",
      "1             -0.18724          -0.14338         -0.364716   \n",
      "2             -0.18724          -0.14338         -0.364716   \n",
      "3             -0.18724          -0.14338         -0.364716   \n",
      "4             -0.18724          -0.14338         -0.364716   \n",
      "\n",
      "   region_warmińsko-mazurskie  region_wielkopolskie  \\\n",
      "0                   -0.159271             -0.269119   \n",
      "1                   -0.159271             -0.269119   \n",
      "2                   -0.159271             -0.269119   \n",
      "3                   -0.159271             -0.269119   \n",
      "4                   -0.159271             -0.269119   \n",
      "\n",
      "   region_zachodniopomorskie  region_łódzkie  region_śląskie  \\\n",
      "0                  -0.340587       -0.204729       -0.333781   \n",
      "1                  -0.340587       -0.204729       -0.333781   \n",
      "2                  -0.340587       -0.204729        2.995978   \n",
      "3                  -0.340587       -0.204729       -0.333781   \n",
      "4                  -0.340587       -0.204729       -0.333781   \n",
      "\n",
      "   region_świętokrzyskie  \n",
      "0              -0.118755  \n",
      "1              -0.118755  \n",
      "2              -0.118755  \n",
      "3              -0.118755  \n",
      "4              -0.118755  \n",
      "\n",
      "[5 rows x 71 columns]\n"
     ]
    }
   ],
   "source": [
    "# Application of the function\n",
    "X_train_scaled, X_test_scaled = scale_train_test(X_train_encoded, X_test_encoded)\n",
    "\n",
    "# View the changes that have made to the data\n",
    "print(X_train_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN_impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNN_impute_train_test(X_train_to_impute, X_test_to_impute,\n",
    "                          features_for_kNN_impute, features_to_onehotencode):       \n",
    "    \n",
    "    \"\"\"\n",
    "    Performs K-Nearest Neighbors (KNN) imputation on selected features in training and test datasets.\n",
    "\n",
    "    This function uses the KNNImputer to fill in missing values in specified features by\n",
    "    leveraging similarity between instances. It also automatically handles one-hot encoded\n",
    "    categorical variables by including all derived columns in the imputation process.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train_to_impute : pd.DataFrame\n",
    "        Training dataset containing missing values to be imputed.\n",
    "\n",
    "    X_test_to_impute : pd.DataFrame\n",
    "        Test dataset with the same structure as the training data.\n",
    "\n",
    "    features_for_kNN_impute : list of str\n",
    "        List of original feature names to apply KNN imputation to.\n",
    "        If a feature has been one-hot encoded, columns after encoding\n",
    "        will be included in the imputation process.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X_train : pd.DataFrame\n",
    "        Training dataset with missing values imputed using KNN.\n",
    "\n",
    "    X_test : pd.DataFrame\n",
    "        Test dataset imputed using the same fitted KNN model from training data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create copies of the original datasets to avoid modifying them in-place\n",
    "    X_train = X_train_to_impute.copy()\n",
    "    X_test = X_test_to_impute.copy()\n",
    "    features_for_kNN_impute_new = features_for_kNN_impute.copy()\n",
    "    \n",
    "    # Identify which features were one-hot encoded\n",
    "    encoded_vars = list(set(features_for_kNN_impute) & set(features_to_onehotencode))\n",
    "    \n",
    "    # Replace original variable name with all derived one-hot encoded columns\n",
    "    for var in encoded_vars:\n",
    "        col_after_encode = [col for col in X_train.columns if col.startswith(var)]\n",
    "        features_for_kNN_impute_new.remove(var)\n",
    "        features_for_kNN_impute_new = features_for_kNN_impute_new + col_after_encode\n",
    "        \n",
    "    # Create the imputer\n",
    "    imputer = KNNImputer()\n",
    "    \n",
    "    # Fit and apply KNN imputation to the selected columns\n",
    "    X_train[features_for_kNN_impute_new] = imputer.fit_transform(X_train[features_for_kNN_impute_new])\n",
    "    X_test[features_for_kNN_impute_new] = imputer.transform(X_test[features_for_kNN_impute_new])\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application of the function\n",
    "X_train_imputed, X_test_imputed = kNN_impute_train_test(X_train_scaled, X_test_scaled,\n",
    "                                                        features_for_kNN_impute, features_to_onehotencode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are there missing values in data? False\n",
      "       area  num_rooms  parking_coded      year  rent_cat  \\\n",
      "0 -0.148860   0.403396       0.919684  0.728266 -0.613762   \n",
      "1  1.920864   0.403396       0.919684  0.696106  0.079673   \n",
      "2  1.040200   0.403396      -1.087330  0.696106 -0.960480   \n",
      "3 -0.488189  -0.714575      -1.087330 -0.365186  0.773109   \n",
      "4 -0.811584  -0.714575       0.919684  0.760427 -0.960480   \n",
      "\n",
      "   number_floor_in_building  ap_floor  utilities_telewizja kablowa  \\\n",
      "0                  0.471969  1.152787                    -0.839817   \n",
      "1                 -0.064076  0.516527                     1.190736   \n",
      "2                 -1.212746 -0.543905                    -0.839817   \n",
      "3                  1.084593 -0.543905                     1.190736   \n",
      "4                 -0.064076 -0.013689                     1.190736   \n",
      "\n",
      "   utilities_internet  utilities_telefon  ...  region_małopolskie  \\\n",
      "0           -1.023096          -0.564978  ...           -0.307281   \n",
      "1            0.977425           1.769981  ...            3.254351   \n",
      "2           -1.023096          -0.564978  ...           -0.307281   \n",
      "3            0.977425          -0.564978  ...           -0.307281   \n",
      "4            0.977425           1.769981  ...            3.254351   \n",
      "\n",
      "   region_podkarpackie  region_podlaskie  region_pomorskie  \\\n",
      "0             -0.18724          -0.14338         -0.364716   \n",
      "1             -0.18724          -0.14338         -0.364716   \n",
      "2             -0.18724          -0.14338         -0.364716   \n",
      "3             -0.18724          -0.14338         -0.364716   \n",
      "4             -0.18724          -0.14338         -0.364716   \n",
      "\n",
      "   region_warmińsko-mazurskie  region_wielkopolskie  \\\n",
      "0                   -0.159271             -0.269119   \n",
      "1                   -0.159271             -0.269119   \n",
      "2                   -0.159271             -0.269119   \n",
      "3                   -0.159271             -0.269119   \n",
      "4                   -0.159271             -0.269119   \n",
      "\n",
      "   region_zachodniopomorskie  region_łódzkie  region_śląskie  \\\n",
      "0                  -0.340587       -0.204729       -0.333781   \n",
      "1                  -0.340587       -0.204729       -0.333781   \n",
      "2                  -0.340587       -0.204729        2.995978   \n",
      "3                  -0.340587       -0.204729       -0.333781   \n",
      "4                  -0.340587       -0.204729       -0.333781   \n",
      "\n",
      "   region_świętokrzyskie  \n",
      "0              -0.118755  \n",
      "1              -0.118755  \n",
      "2              -0.118755  \n",
      "3              -0.118755  \n",
      "4              -0.118755  \n",
      "\n",
      "[5 rows x 71 columns]\n"
     ]
    }
   ],
   "source": [
    "# View the changes that have made to the data\n",
    "print(\"Are there missing values in data?\", X_train_imputed.isna().any().any())\n",
    "print(X_train_imputed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare_train_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_test_data (data, test_size, random_seed):\n",
    "    \n",
    "    \"\"\"\n",
    "    Prepares and processes the dataset for model training and evaluation by applying\n",
    "    feature selection, encoding, scaling, and missing value imputation.\n",
    "\n",
    "    The function loads preprocessing configuration from a saved dictionary and performs the following steps:\n",
    "    1. Splits the dataset into train and test sets.\n",
    "    2. Cleans missing and outlier values in train and test sets.\n",
    "    3. Selects relevant features and target column.\n",
    "    3. Applies one-hot encoding to categorical features.\n",
    "    4. Scales numeric features using StandardScaler.\n",
    "    5. Imputes missing values using K-Nearest Neighbors imputation.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        The full dataset to be prepared for training and testing.\n",
    "\n",
    "    test_size : float\n",
    "        Proportion of the dataset to include in the test split (e.g., 0.2 for 20%).\n",
    "\n",
    "    random_seed : int\n",
    "        Random seed to ensure reproducibility of the train-test split.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X_train_imputed : pd.DataFrame\n",
    "        Fully preprocessed training feature set.\n",
    "\n",
    "    X_test_imputed : pd.DataFrame\n",
    "        Fully preprocessed test feature set.\n",
    "\n",
    "    y_train : pd.Series\n",
    "        Target variable for training data.\n",
    "\n",
    "    y_test : pd.Series\n",
    "        Target variable for test data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load preprocessing configuration with selected features and target definition\n",
    "    model_features_dict = joblib.load(\"1. Data Preparation/model_features_dict.joblib\")\n",
    "    \n",
    "    features_to_use = model_features_dict['features_to_use']\n",
    "    target = model_features_dict['target']\n",
    "    features_to_onehotencode = model_features_dict['features_to_onehotencode']\n",
    "    features_for_kNN_impute = model_features_dict['features_for_kNN_impute']\n",
    "    \n",
    "    # Validate that all required features exist in the provided dataset\n",
    "    if all(x in data.columns for x in model_features_dict['features_to_use']) == 0:\n",
    "        print(\"The dataset does not have all the variables defined for modeling\")\n",
    "    \n",
    "    # Split the dataset into training and testing sets\n",
    "    train_data, test_data = train_test_split(preliminary_transformed_data,\n",
    "                                                    test_size = test_size,\n",
    "                                                    random_state = random_seed)\n",
    "\n",
    "    # Clean the train and test data\n",
    "    train_cleaned_data = cleaning_data(train_data, train_dataset = True)\n",
    "    test_cleaned_data = cleaning_data(test_data, train_dataset = False)\n",
    "\n",
    "    # Extract features (X) and target variable (y)\n",
    "    X_train = train_cleaned_data[features_to_use]\n",
    "    y_train = train_cleaned_data[target]\n",
    "\n",
    "    X_test = test_cleaned_data[features_to_use]\n",
    "    y_test = test_cleaned_data[target]\n",
    "    \n",
    "    # Ensure NaNs are np.nan, not pd.NA\n",
    "    X_train = X_train.replace({pd.NA: np.nan})\n",
    "    X_test = X_test.replace({pd.NA: np.nan})\n",
    "    \n",
    "    # One-hot encode categorical features\n",
    "    X_train_encoded, X_test_encoded = one_hot_encode_train_test(X_train, X_test, features_to_onehotencode)\n",
    "    \n",
    "    # Scale features using StandardScaler\n",
    "    X_train_scaled, X_test_scaled = scale_train_test(X_train_encoded, X_test_encoded)\n",
    "    \n",
    "    # Impute missing values using KNN imputer\n",
    "    X_train_imputed, X_test_imputed = kNN_impute_train_test(X_train_scaled, X_test_scaled,\n",
    "                                                            features_for_kNN_impute, features_to_onehotencode)\n",
    "    \n",
    "    return X_train_imputed, X_test_imputed, y_train, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                  W zbiorze wystąpiły braki w cechach: 'typ budynku', 'powierzchnia',\n",
      "                  'liczba pokoi', 'rynek', 'ogłoszeniodawca', 'miasto', 'województwo'.\n",
      "                  Do otrzymania prognozy wszystkie z powyższych cech\n",
      "                  muszą być wypełnione.\n",
      "                  Obserwacje te zostały usunięte ze zbioru do predykcji.\n",
      "                  \n",
      "\n",
      "                  W zbiorze wystąpiły obserwację o skrajnych wartościach\n",
      "                  ze względu na zaproponowaną cenę, powierzchnię, piętro lub rok budynku.\n",
      "                  Może zmniejszeć dokładność prognozy.\n",
      "                  \n"
     ]
    }
   ],
   "source": [
    "# Application of the function\n",
    "X_train, X_test, y_train, y_test = prepare_train_test_data(preliminary_transformed_data,\n",
    "                                                           test_size = 0.2, random_seed = 99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 33509 entries, 0 to 33508\n",
      "Data columns (total 71 columns):\n",
      " #   Column                                Non-Null Count  Dtype  \n",
      "---  ------                                --------------  -----  \n",
      " 0   area                                  33509 non-null  float64\n",
      " 1   num_rooms                             33509 non-null  float64\n",
      " 2   parking_coded                         33509 non-null  float64\n",
      " 3   year                                  33509 non-null  float64\n",
      " 4   rent_cat                              33509 non-null  float64\n",
      " 5   number_floor_in_building              33509 non-null  float64\n",
      " 6   ap_floor                              33509 non-null  float64\n",
      " 7   utilities_telewizja kablowa           33509 non-null  float64\n",
      " 8   utilities_internet                    33509 non-null  float64\n",
      " 9   utilities_telefon                     33509 non-null  float64\n",
      " 10  security_drzwi / okna antywłamaniowe  33509 non-null  float64\n",
      " 11  security_teren zamknięty              33509 non-null  float64\n",
      " 12  security_domofon / wideofon           33509 non-null  float64\n",
      " 13  security_monitoring / ochrona         33509 non-null  float64\n",
      " 14  security_rolety antywłamaniowe        33509 non-null  float64\n",
      " 15  security_system alarmowy              33509 non-null  float64\n",
      " 16  equipment_zmywarka                    33509 non-null  float64\n",
      " 17  equipment_lodówka                     33509 non-null  float64\n",
      " 18  equipment_meble                       33509 non-null  float64\n",
      " 19  equipment_piekarnik                   33509 non-null  float64\n",
      " 20  equipment_kuchenka                    33509 non-null  float64\n",
      " 21  equipment_pralka                      33509 non-null  float64\n",
      " 22  equipment_telewizor                   33509 non-null  float64\n",
      " 23  add_inf_pom. użytkowe                 33509 non-null  float64\n",
      " 24  add_inf_piwnica                       33509 non-null  float64\n",
      " 25  add_inf_dwupoziomowe                  33509 non-null  float64\n",
      " 26  add_inf_oddzielna kuchnia             33509 non-null  float64\n",
      " 27  add_inf_klimatyzacja                  33509 non-null  float64\n",
      " 28  perks_balkon                          33509 non-null  float64\n",
      " 29  perks_taras                           33509 non-null  float64\n",
      " 30  perks_ogródek                         33509 non-null  float64\n",
      " 31  with_powiat_rights                    33509 non-null  float64\n",
      " 32  pop_numb_cat                          33509 non-null  float64\n",
      " 33  pop_dens_cat                          33509 non-null  float64\n",
      " 34  ownership_status_inny                 33509 non-null  float64\n",
      " 35  ownership_status_pełna własność       33509 non-null  float64\n",
      " 36  flat_condition_do remontu             33509 non-null  float64\n",
      " 37  flat_condition_do wykończenia         33509 non-null  float64\n",
      " 38  flat_condition_do zamieszkania        33509 non-null  float64\n",
      " 39  heating_gazowe                        33509 non-null  float64\n",
      " 40  heating_inny                          33509 non-null  float64\n",
      " 41  heating_miejskie                      33509 non-null  float64\n",
      " 42  market_pierwotny                      33509 non-null  float64\n",
      " 43  ad_type_biuro nieruchomości           33509 non-null  float64\n",
      " 44  ad_type_deweloper                     33509 non-null  float64\n",
      " 45  windows_inny                          33509 non-null  float64\n",
      " 46  windows_plastikowe                    33509 non-null  float64\n",
      " 47  lift_tak                              33509 non-null  float64\n",
      " 48  mater_cegła                           33509 non-null  float64\n",
      " 49  mater_inny                            33509 non-null  float64\n",
      " 50  mater_pustak                          33509 non-null  float64\n",
      " 51  mater_silikat                         33509 non-null  float64\n",
      " 52  mater_wielka płyta                    33509 non-null  float64\n",
      " 53  devel_type_apartamentowiec            33509 non-null  float64\n",
      " 54  devel_type_blok                       33509 non-null  float64\n",
      " 55  devel_type_kamienica                  33509 non-null  float64\n",
      " 56  region_dolnośląskie                   33509 non-null  float64\n",
      " 57  region_kujawsko-pomorskie             33509 non-null  float64\n",
      " 58  region_lubelskie                      33509 non-null  float64\n",
      " 59  region_lubuskie                       33509 non-null  float64\n",
      " 60  region_mazowieckie                    33509 non-null  float64\n",
      " 61  region_małopolskie                    33509 non-null  float64\n",
      " 62  region_podkarpackie                   33509 non-null  float64\n",
      " 63  region_podlaskie                      33509 non-null  float64\n",
      " 64  region_pomorskie                      33509 non-null  float64\n",
      " 65  region_warmińsko-mazurskie            33509 non-null  float64\n",
      " 66  region_wielkopolskie                  33509 non-null  float64\n",
      " 67  region_zachodniopomorskie             33509 non-null  float64\n",
      " 68  region_łódzkie                        33509 non-null  float64\n",
      " 69  region_śląskie                        33509 non-null  float64\n",
      " 70  region_świętokrzyskie                 33509 non-null  float64\n",
      "dtypes: float64(71)\n",
      "memory usage: 18.2 MB\n"
     ]
    }
   ],
   "source": [
    "# View the changes that have made to the data\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing for training finale model\n",
    "Before modeling, the data were processed as follows:\n",
    "1. categorical features were one-hot encoded - **one_hot_encode_train_ecoder**\n",
    "2. all features were scale with z-score normalization - **scale_train_scaler**\n",
    "3. missing data was filled usin kNN imputation - **kNN_impute_train_imputer**\n",
    "4. all the steps above were combined, and the fitted encoder, scaler and imputer were saved- **prepare_final_training_data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one_hot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_train_encoder(X_to_encode, features_to_onehotencode):\n",
    "    \n",
    "    \"\"\"\n",
    "    Applies one-hot encoding to specified categorical features in the training dataset\n",
    "    and returns both the transformed DataFrame and a dictionary of trained encoders.\n",
    "\n",
    "    For each categorical feature, the function:\n",
    "    - Checks whether specific unwanted categories (e.g., 'nie podano') are present.\n",
    "    - Drops the first matching category from the one-hot encoding if found, otherwise drops the first by default.\n",
    "    - Uses sklearn's OneHotEncoder with 'handle_unknown=ignore' to handle unseen categories gracefully.\n",
    "    - Returns a transformed DataFrame and a dictionary of fitted encoders for future use (e.g., for encoding test data).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_to_encode : pd.DataFrame\n",
    "        Input DataFrame containing categorical features to be encoded.\n",
    "\n",
    "    features_to_onehotencode : list of str\n",
    "        List of column names in `X_to_encode` to apply one-hot encoding on.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X : pd.DataFrame\n",
    "        DataFrame with original categorical features replaced by one-hot encoded columns.\n",
    "\n",
    "    encoders : dict\n",
    "        Dictionary mapping each encoded feature name to its corresponding fitted OneHotEncoder instance.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create copy of the original dataset to avoid modifying it in-place\n",
    "    X = X_to_encode.copy()\n",
    "    \n",
    "    # Dictionary to store fitted encoders for each column\n",
    "    encoders = {}\n",
    "    \n",
    "    for column in features_to_onehotencode:\n",
    "        # Define categories that should be dropped if they exist\n",
    "        categories_to_drop = ['nie podano', 'inny', 'wtórny', 'prywatny', 'opolskie']\n",
    "        \n",
    "        # Determine which category to drop (first match found)\n",
    "        cat_to_drop = None\n",
    "        \n",
    "        for cat in categories_to_drop:\n",
    "            if cat in X[column].unique():\n",
    "                cat_to_drop = [cat]\n",
    "                break\n",
    "                \n",
    "        # Create and fit the encoder\n",
    "        encoder = OneHotEncoder(drop = cat_to_drop if cat_to_drop else 'first', sparse=False, handle_unknown='ignore')\n",
    "        encoder.fit(X[[column]])\n",
    "        \n",
    "        # Get names of one-hot encoded columns\n",
    "        cols = encoder.get_feature_names_out([column])\n",
    "        \n",
    "        # Transform the data and wrap in DataFrame\n",
    "        X_encoded = pd.DataFrame(encoder.transform(X[[column]]), columns=cols, index=X.index)\n",
    "        \n",
    "        # Append the encoded columns to the dataset\n",
    "        X = pd.concat([X, X_encoded], axis=1)\n",
    "        \n",
    "        # Save the encoder for this column\n",
    "        encoders[column] = encoder\n",
    "        \n",
    "    # Drop the original categorical column\n",
    "    X.drop(features_to_onehotencode, axis=1, inplace=True)\n",
    "    \n",
    "    return X, encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_train_scaler(X_to_scale):\n",
    "    \n",
    "    \"\"\"\n",
    "    Applies standard scaling (zero mean, unit variance) to the input dataset and returns\n",
    "    both the scaled dataset and the fitted scaler for future use (e.g., for test data).\n",
    "\n",
    "    The function performs the following steps:\n",
    "    - Copies the input DataFrame to avoid modifying the original.\n",
    "    - Fits a 'StandardScaler' from 'sklearn.preprocessing' to the data.\n",
    "    - Applies the transformation and returns the scaled data along with the scaler.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_to_scale : pd.DataFrame\n",
    "        The DataFrame containing numerical features to be scaled.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X : pd.DataFrame\n",
    "        The scaled version of the input DataFrame, with the same column names.\n",
    "\n",
    "    scaler : StandardScaler\n",
    "        The fitted 'StandardScaler' object, which can be used to transform new data\n",
    "        (e.g., test set) using the same scaling parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create copy of the original dataset to avoid modifying it in-place\n",
    "    X = X_to_scale.copy()\n",
    "    \n",
    "    # Create the scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit and apply the scaler to the data and preserve column names\n",
    "    X = pd.DataFrame(scaler.fit_transform(X), columns = X.columns)\n",
    "        \n",
    "    return X, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN_impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNN_impute_train_imputer(X_to_impute, features_for_kNN_impute, features_to_onehotencode):       \n",
    "    \n",
    "    \"\"\"\n",
    "    Performs K-Nearest Neighbors (KNN) imputation on selected features of a dataset,\n",
    "    including those that have been one-hot encoded.\n",
    "\n",
    "    The function:\n",
    "    - Identifies features that were originally categorical and have been one-hot encoded.\n",
    "    - Replaces these original variables in the list of features to impute with their\n",
    "      one-hot encoded column names.\n",
    "    - Fits a 'KNNImputer' on the specified features.\n",
    "    - Returns the dataset with imputed values and a dictionary containing the fitted\n",
    "      imputer and the actual list of features used.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_to_impute : pd.DataFrame\n",
    "        The input dataset (typically already encoded and scaled) with missing values to be imputed.\n",
    "\n",
    "    features_for_kNN_impute : list of str\n",
    "        List of original features intended for imputation.\n",
    "\n",
    "    features_to_onehotencode : list of str\n",
    "        List of features that were one-hot encoded (to help expand into multiple columns).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X : pd.DataFrame\n",
    "        The dataset with imputed values for the specified features.\n",
    "\n",
    "    imputer_dict : dict\n",
    "        A dictionary containing:\n",
    "            - 'imputer' : the fitted 'KNNImputer'`object\n",
    "            - 'features': the actual list of column names used for imputation,\n",
    "                          including one-hot encoded columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create copy of the original dataset to avoid modifying it in-place\n",
    "    X = X_to_impute.copy()\n",
    "    \n",
    "     # Copy the list of features to impute\n",
    "    features_for_kNN_impute_new = features_for_kNN_impute.copy()\n",
    "    \n",
    "    # Identify one-hot encoded variables among features to impute\n",
    "    encoded_vars = list(set(features_for_kNN_impute) & set(features_to_onehotencode))\n",
    "    \n",
    "    # Replace encoded variables with their actual one-hot encoded column names\n",
    "    for var in encoded_vars:\n",
    "        col_after_encode = [col for col in X.columns if col.startswith(var)]\n",
    "        features_for_kNN_impute_new.remove(var)\n",
    "        features_for_kNN_impute_new = features_for_kNN_impute_new + col_after_encode\n",
    "        \n",
    "    # Create the imputer\n",
    "    imputer = KNNImputer()\n",
    "    \n",
    "    # Fit and apply imputation only on the selected columns\n",
    "    X[features_for_kNN_impute_new] = imputer.fit_transform(X[features_for_kNN_impute_new])\n",
    "    \n",
    "    # Return the transformed data and the fitted imputer with the column names used\n",
    "    imputer_dict = {'imputer': imputer,\n",
    "                    'features': features_for_kNN_impute_new}\n",
    "    \n",
    "    return X, imputer_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare_final_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_final_training_data(data_to_train, to_save = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Prepares the final training dataset by applying a complete preprocessing pipeline,\n",
    "    including one-hot encoding, scaling, and KNN imputation.\n",
    "\n",
    "    This function:\n",
    "    - Loads the model configuration (features, encoders) from a predefined dictionary.\n",
    "    - Applies preprocessing steps in the following order:\n",
    "        1. Cleans missing and outlier values.\n",
    "        2. One-hot encoding of categorical variables.\n",
    "        3. Feature standardization (z-score scaling).\n",
    "        4. Missing data imputation using K-Nearest Neighbors.\n",
    "    - Optionally saves the fitted preprocessing objects (encoder, scaler, imputer) for future use.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        The raw input dataset containing both features and the target variable.\n",
    "\n",
    "    to_save : bool, default=True\n",
    "        If True, saves the fitted encoder, scaler, and imputer to disk under the\n",
    "        folder '1. Data Preparation/pipeline_objects'.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X_imputed : pd.DataFrame\n",
    "        The fully preprocessed feature matrix ready for training.\n",
    "\n",
    "    y : pd.Series\n",
    "        The target variable extracted from the input data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the list of features and settings used for preprocessing\n",
    "    model_features_dict = joblib.load(\"1. Data Preparation/model_features_dict.joblib\")\n",
    "    \n",
    "    features_to_use = model_features_dict['features_to_use']\n",
    "    target = model_features_dict['target']\n",
    "    features_to_onehotencode = model_features_dict['features_to_onehotencode']\n",
    "    features_for_kNN_impute = model_features_dict['features_for_kNN_impute']\n",
    "    \n",
    "    # Check whether all required features are present in the data\n",
    "    if all(x in data_to_train.columns for x in model_features_dict['features_to_use']) == 0:\n",
    "        print(\"The dataset does not have all the variables defined for modeling\")\n",
    "    \n",
    "    # Copy the dataset to avoid in-place changes\n",
    "    data = data_to_train.copy()\n",
    "    \n",
    "    # Proceed missing and outlier values\n",
    "    data = cleaning_data(data, train_dataset = True)\n",
    "    \n",
    "    # Ensure compatibility with np.nan\n",
    "    data = data.replace({pd.NA: np.nan})\n",
    "    \n",
    "    # Separate input features and target\n",
    "    X = data[features_to_use]\n",
    "    y = data[target]\n",
    "    \n",
    "    # One-hot encode categorical features\n",
    "    X_encoded, encoder = one_hot_encode_train_encoder(X, features_to_onehotencode)\n",
    "    \n",
    "    # Scale features using StandardScaler\n",
    "    X_scaled, scaler = scale_train_scaler(X_encoded)\n",
    "    \n",
    "    # Impute missing values using KNN imputer\n",
    "    X_imputed, imputer = kNN_impute_train_imputer(X_scaled, features_for_kNN_impute, features_to_onehotencode)\n",
    "    \n",
    "    # Optionally save preprocessing objects to disk for future use\n",
    "    if to_save: \n",
    "        folder_path = \"1. Data Preparation/pipeline_objects\"\n",
    "        joblib.dump(encoder, f\"{folder_path}/encoder.pkl\")\n",
    "        joblib.dump(scaler, f\"{folder_path}/scaler.pkl\")\n",
    "        joblib.dump(imputer, f\"{folder_path}/imputer.pkl\")\n",
    "    \n",
    "    return X_imputed, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application of the function\n",
    "X_final, y_final = prepare_final_training_data(cleaned_outliers_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>area</th>\n",
       "      <th>num_rooms</th>\n",
       "      <th>parking_coded</th>\n",
       "      <th>year</th>\n",
       "      <th>rent_cat</th>\n",
       "      <th>number_floor_in_building</th>\n",
       "      <th>ap_floor</th>\n",
       "      <th>utilities_telewizja kablowa</th>\n",
       "      <th>utilities_internet</th>\n",
       "      <th>utilities_telefon</th>\n",
       "      <th>...</th>\n",
       "      <th>region_małopolskie</th>\n",
       "      <th>region_podkarpackie</th>\n",
       "      <th>region_podlaskie</th>\n",
       "      <th>region_pomorskie</th>\n",
       "      <th>region_warmińsko-mazurskie</th>\n",
       "      <th>region_wielkopolskie</th>\n",
       "      <th>region_zachodniopomorskie</th>\n",
       "      <th>region_łódzkie</th>\n",
       "      <th>region_śląskie</th>\n",
       "      <th>region_świętokrzyskie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.957881</td>\n",
       "      <td>-0.717919</td>\n",
       "      <td>0.919302</td>\n",
       "      <td>0.695154</td>\n",
       "      <td>0.082790</td>\n",
       "      <td>-0.444982</td>\n",
       "      <td>-0.544803</td>\n",
       "      <td>1.190836</td>\n",
       "      <td>0.9784</td>\n",
       "      <td>1.773461</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.309063</td>\n",
       "      <td>-0.187996</td>\n",
       "      <td>-0.140732</td>\n",
       "      <td>-0.366746</td>\n",
       "      <td>-0.160163</td>\n",
       "      <td>-0.270693</td>\n",
       "      <td>-0.342017</td>\n",
       "      <td>-0.205613</td>\n",
       "      <td>-0.331419</td>\n",
       "      <td>-0.117386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.498006</td>\n",
       "      <td>0.399811</td>\n",
       "      <td>0.919302</td>\n",
       "      <td>0.598817</td>\n",
       "      <td>0.777088</td>\n",
       "      <td>1.086672</td>\n",
       "      <td>1.047385</td>\n",
       "      <td>1.190836</td>\n",
       "      <td>0.9784</td>\n",
       "      <td>-0.563869</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.309063</td>\n",
       "      <td>-0.187996</td>\n",
       "      <td>-0.140732</td>\n",
       "      <td>-0.366746</td>\n",
       "      <td>-0.160163</td>\n",
       "      <td>-0.270693</td>\n",
       "      <td>-0.342017</td>\n",
       "      <td>4.863508</td>\n",
       "      <td>-0.331419</td>\n",
       "      <td>-0.117386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.100095</td>\n",
       "      <td>0.399811</td>\n",
       "      <td>0.919302</td>\n",
       "      <td>0.695154</td>\n",
       "      <td>-0.958656</td>\n",
       "      <td>-1.210810</td>\n",
       "      <td>-1.075533</td>\n",
       "      <td>1.190836</td>\n",
       "      <td>0.9784</td>\n",
       "      <td>-0.563869</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.309063</td>\n",
       "      <td>-0.187996</td>\n",
       "      <td>-0.140732</td>\n",
       "      <td>-0.366746</td>\n",
       "      <td>-0.160163</td>\n",
       "      <td>-0.270693</td>\n",
       "      <td>-0.342017</td>\n",
       "      <td>-0.205613</td>\n",
       "      <td>-0.331419</td>\n",
       "      <td>-0.117386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.895917</td>\n",
       "      <td>1.517541</td>\n",
       "      <td>0.919302</td>\n",
       "      <td>0.695154</td>\n",
       "      <td>-0.958656</td>\n",
       "      <td>-1.057644</td>\n",
       "      <td>-1.075533</td>\n",
       "      <td>-0.839746</td>\n",
       "      <td>0.9784</td>\n",
       "      <td>-0.563869</td>\n",
       "      <td>...</td>\n",
       "      <td>3.235590</td>\n",
       "      <td>-0.187996</td>\n",
       "      <td>-0.140732</td>\n",
       "      <td>-0.366746</td>\n",
       "      <td>-0.160163</td>\n",
       "      <td>-0.270693</td>\n",
       "      <td>-0.342017</td>\n",
       "      <td>-0.205613</td>\n",
       "      <td>-0.331419</td>\n",
       "      <td>-0.117386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.522817</td>\n",
       "      <td>0.399811</td>\n",
       "      <td>-1.087782</td>\n",
       "      <td>-2.098623</td>\n",
       "      <td>2.512832</td>\n",
       "      <td>-0.062069</td>\n",
       "      <td>0.516656</td>\n",
       "      <td>1.190836</td>\n",
       "      <td>0.9784</td>\n",
       "      <td>1.773461</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.309063</td>\n",
       "      <td>-0.187996</td>\n",
       "      <td>-0.140732</td>\n",
       "      <td>-0.366746</td>\n",
       "      <td>-0.160163</td>\n",
       "      <td>-0.270693</td>\n",
       "      <td>-0.342017</td>\n",
       "      <td>-0.205613</td>\n",
       "      <td>-0.331419</td>\n",
       "      <td>-0.117386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       area  num_rooms  parking_coded      year  rent_cat  \\\n",
       "0 -0.957881  -0.717919       0.919302  0.695154  0.082790   \n",
       "1  0.498006   0.399811       0.919302  0.598817  0.777088   \n",
       "2  0.100095   0.399811       0.919302  0.695154 -0.958656   \n",
       "3  0.895917   1.517541       0.919302  0.695154 -0.958656   \n",
       "4  0.522817   0.399811      -1.087782 -2.098623  2.512832   \n",
       "\n",
       "   number_floor_in_building  ap_floor  utilities_telewizja kablowa  \\\n",
       "0                 -0.444982 -0.544803                     1.190836   \n",
       "1                  1.086672  1.047385                     1.190836   \n",
       "2                 -1.210810 -1.075533                     1.190836   \n",
       "3                 -1.057644 -1.075533                    -0.839746   \n",
       "4                 -0.062069  0.516656                     1.190836   \n",
       "\n",
       "   utilities_internet  utilities_telefon  ...  region_małopolskie  \\\n",
       "0              0.9784           1.773461  ...           -0.309063   \n",
       "1              0.9784          -0.563869  ...           -0.309063   \n",
       "2              0.9784          -0.563869  ...           -0.309063   \n",
       "3              0.9784          -0.563869  ...            3.235590   \n",
       "4              0.9784           1.773461  ...           -0.309063   \n",
       "\n",
       "   region_podkarpackie  region_podlaskie  region_pomorskie  \\\n",
       "0            -0.187996         -0.140732         -0.366746   \n",
       "1            -0.187996         -0.140732         -0.366746   \n",
       "2            -0.187996         -0.140732         -0.366746   \n",
       "3            -0.187996         -0.140732         -0.366746   \n",
       "4            -0.187996         -0.140732         -0.366746   \n",
       "\n",
       "   region_warmińsko-mazurskie  region_wielkopolskie  \\\n",
       "0                   -0.160163             -0.270693   \n",
       "1                   -0.160163             -0.270693   \n",
       "2                   -0.160163             -0.270693   \n",
       "3                   -0.160163             -0.270693   \n",
       "4                   -0.160163             -0.270693   \n",
       "\n",
       "   region_zachodniopomorskie  region_łódzkie  region_śląskie  \\\n",
       "0                  -0.342017       -0.205613       -0.331419   \n",
       "1                  -0.342017        4.863508       -0.331419   \n",
       "2                  -0.342017       -0.205613       -0.331419   \n",
       "3                  -0.342017       -0.205613       -0.331419   \n",
       "4                  -0.342017       -0.205613       -0.331419   \n",
       "\n",
       "   region_świętokrzyskie  \n",
       "0              -0.117386  \n",
       "1              -0.117386  \n",
       "2              -0.117386  \n",
       "3              -0.117386  \n",
       "4              -0.117386  \n",
       "\n",
       "[5 rows x 71 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the changes that have made to the data\n",
    "X_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing used on the production function\n",
    "For the prediction, using saved preprocessing pipeline objectes the data were processed by **prepare_production_modeling_data** function as follows:\n",
    "1. categorical features were one-hot encoded\n",
    "2. all features were scale with z-score normalization\n",
    "3. missing data was filled usin kNN imputation\n",
    "4. all the steps above were combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_production_modeling_data(data_to_predict):\n",
    "    \n",
    "    \"\"\"\n",
    "    Prepares production (inference) data by applying a saved preprocessing pipeline objectes.\n",
    "\n",
    "    This function:\n",
    "    1. Loads the saved model preprocessing objects (encoders, scaler, and imputer).\n",
    "    2. Proceeds missing and outlier values.\n",
    "    3. Applies one-hot encoding using pre-fitted encoders.\n",
    "    4. Scales the features using the pre-fitted StandardScaler.\n",
    "    5. Performs KNN imputation for missing values using the saved imputer.\n",
    "\n",
    "    This ensures consistency with the transformations applied during model training.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        The new dataset (e.g., from production or deployment environment) containing both\n",
    "        input features and the target variable.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X : pd.DataFrame\n",
    "        The fully preprocessed feature matrix ready for inference.\n",
    "\n",
    "    y : pd.Series\n",
    "        The target variable from the provided data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load feature definitions and preprocessing components\n",
    "    model_features_dict = joblib.load(\"1. Data Preparation/model_features_dict.joblib\")\n",
    "    \n",
    "    features_to_use = model_features_dict['features_to_use']\n",
    "    target = model_features_dict['target']\n",
    "    features_to_onehotencode = model_features_dict['features_to_onehotencode']\n",
    "    \n",
    "    folder_path = \"1. Data Preparation/pipeline_objects\"\n",
    "    features_for_kNN_impute = joblib.load(f\"{folder_path}/imputer.pkl\")['features']\n",
    "    \n",
    "    encoder = joblib.load(f\"{folder_path}/encoder.pkl\")\n",
    "    scaler = joblib.load(f\"{folder_path}/scaler.pkl\")\n",
    "    imputer = joblib.load(f\"{folder_path}/imputer.pkl\")['imputer']\n",
    "    \n",
    "    # Copy the dataset to avoid in-place changes\n",
    "    data = data_to_predict.copy()\n",
    "    \n",
    "    # Proceed missing and outlier values\n",
    "    data = cleaning_data(data, train_dataset = False)\n",
    "    \n",
    "    # Ensure compatibility with np.nan\n",
    "    data = data.replace({pd.NA: np.nan})\n",
    "    \n",
    "    # Separate input features and target\n",
    "    X = data[features_to_use]\n",
    "    y = data[target]\n",
    "    \n",
    "    # Step 1: Apply saved one-hot encoders for each categorical column\n",
    "    for column in features_to_onehotencode:\n",
    "        \n",
    "        encoder_col = encoder[column]   \n",
    "        # Column names for output\n",
    "        cols = encoder_col.get_feature_names_out([column])\n",
    "        \n",
    "        X_encoded = pd.DataFrame(encoder_col.transform(X[[column]]), columns=cols, index=X.index)\n",
    "        \n",
    "        X = pd.concat([X, X_encoded], axis=1)\n",
    "        \n",
    "    # Drop original categorical columns after encoding\n",
    "    X.drop(features_to_onehotencode, axis=1, inplace=True)\n",
    "    \n",
    "    # Step 2: Scale all features using saved StandardScaler\n",
    "    X = pd.DataFrame(scaler.transform(X), columns = X.columns)\n",
    "    \n",
    "    # Step 3: Impute missing values using saved KNNImputer\n",
    "    X[features_for_kNN_impute] = imputer.transform(X[features_for_kNN_impute])\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application of the function\n",
    "X, y = prepare_production_modeling_data(cleaned_outliers_data.iloc[5:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 995 entries, 0 to 994\n",
      "Data columns (total 71 columns):\n",
      " #   Column                                Non-Null Count  Dtype  \n",
      "---  ------                                --------------  -----  \n",
      " 0   area                                  995 non-null    float64\n",
      " 1   num_rooms                             995 non-null    float64\n",
      " 2   parking_coded                         995 non-null    float64\n",
      " 3   year                                  995 non-null    float64\n",
      " 4   rent_cat                              995 non-null    float64\n",
      " 5   number_floor_in_building              995 non-null    float64\n",
      " 6   ap_floor                              995 non-null    float64\n",
      " 7   utilities_telewizja kablowa           995 non-null    float64\n",
      " 8   utilities_internet                    995 non-null    float64\n",
      " 9   utilities_telefon                     995 non-null    float64\n",
      " 10  security_drzwi / okna antywłamaniowe  995 non-null    float64\n",
      " 11  security_teren zamknięty              995 non-null    float64\n",
      " 12  security_domofon / wideofon           995 non-null    float64\n",
      " 13  security_monitoring / ochrona         995 non-null    float64\n",
      " 14  security_rolety antywłamaniowe        995 non-null    float64\n",
      " 15  security_system alarmowy              995 non-null    float64\n",
      " 16  equipment_zmywarka                    995 non-null    float64\n",
      " 17  equipment_lodówka                     995 non-null    float64\n",
      " 18  equipment_meble                       995 non-null    float64\n",
      " 19  equipment_piekarnik                   995 non-null    float64\n",
      " 20  equipment_kuchenka                    995 non-null    float64\n",
      " 21  equipment_pralka                      995 non-null    float64\n",
      " 22  equipment_telewizor                   995 non-null    float64\n",
      " 23  add_inf_pom. użytkowe                 995 non-null    float64\n",
      " 24  add_inf_piwnica                       995 non-null    float64\n",
      " 25  add_inf_dwupoziomowe                  995 non-null    float64\n",
      " 26  add_inf_oddzielna kuchnia             995 non-null    float64\n",
      " 27  add_inf_klimatyzacja                  995 non-null    float64\n",
      " 28  perks_balkon                          995 non-null    float64\n",
      " 29  perks_taras                           995 non-null    float64\n",
      " 30  perks_ogródek                         995 non-null    float64\n",
      " 31  with_powiat_rights                    995 non-null    float64\n",
      " 32  pop_numb_cat                          995 non-null    float64\n",
      " 33  pop_dens_cat                          995 non-null    float64\n",
      " 34  ownership_status_inny                 995 non-null    float64\n",
      " 35  ownership_status_pełna własność       995 non-null    float64\n",
      " 36  flat_condition_do remontu             995 non-null    float64\n",
      " 37  flat_condition_do wykończenia         995 non-null    float64\n",
      " 38  flat_condition_do zamieszkania        995 non-null    float64\n",
      " 39  heating_gazowe                        995 non-null    float64\n",
      " 40  heating_inny                          995 non-null    float64\n",
      " 41  heating_miejskie                      995 non-null    float64\n",
      " 42  market_pierwotny                      995 non-null    float64\n",
      " 43  ad_type_biuro nieruchomości           995 non-null    float64\n",
      " 44  ad_type_deweloper                     995 non-null    float64\n",
      " 45  windows_inny                          995 non-null    float64\n",
      " 46  windows_plastikowe                    995 non-null    float64\n",
      " 47  lift_tak                              995 non-null    float64\n",
      " 48  mater_cegła                           995 non-null    float64\n",
      " 49  mater_inny                            995 non-null    float64\n",
      " 50  mater_pustak                          995 non-null    float64\n",
      " 51  mater_silikat                         995 non-null    float64\n",
      " 52  mater_wielka płyta                    995 non-null    float64\n",
      " 53  devel_type_apartamentowiec            995 non-null    float64\n",
      " 54  devel_type_blok                       995 non-null    float64\n",
      " 55  devel_type_kamienica                  995 non-null    float64\n",
      " 56  region_dolnośląskie                   995 non-null    float64\n",
      " 57  region_kujawsko-pomorskie             995 non-null    float64\n",
      " 58  region_lubelskie                      995 non-null    float64\n",
      " 59  region_lubuskie                       995 non-null    float64\n",
      " 60  region_mazowieckie                    995 non-null    float64\n",
      " 61  region_małopolskie                    995 non-null    float64\n",
      " 62  region_podkarpackie                   995 non-null    float64\n",
      " 63  region_podlaskie                      995 non-null    float64\n",
      " 64  region_pomorskie                      995 non-null    float64\n",
      " 65  region_warmińsko-mazurskie            995 non-null    float64\n",
      " 66  region_wielkopolskie                  995 non-null    float64\n",
      " 67  region_zachodniopomorskie             995 non-null    float64\n",
      " 68  region_łódzkie                        995 non-null    float64\n",
      " 69  region_śląskie                        995 non-null    float64\n",
      " 70  region_świętokrzyskie                 995 non-null    float64\n",
      "dtypes: float64(71)\n",
      "memory usage: 552.0 KB\n"
     ]
    }
   ],
   "source": [
    "# View the changes that have made to the data\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting coordinates from address data for visualization on the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_address_string(address_str):\n",
    "    \n",
    "    \"\"\"\n",
    "    Cleans and standardizes an address string by removing common street prefixes \n",
    "    and reducing redundant whitespace.\n",
    "\n",
    "    This function is particularly useful for ensuring address consistency \n",
    "    when performing string comparisons, matching, or geocoding.\n",
    "\n",
    "    Operations performed:\n",
    "    - Removes common Polish street prefixes (e.g., \"ul.\", \"ulica\", \"Ul\", etc.).\n",
    "    - Collapses multiple spaces into a single space.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    address_str : str\n",
    "        The original address string to clean and normalize.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    address_str : str\n",
    "        The cleaned and standardized address string.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define unwanted prefixes to remove from address string\n",
    "    substrings_to_delete = (\"ulica \", \"Ulica \", \"ul. \", \"ul \", \"Ul. \", \"Ul \", \"ul.\")\n",
    "    \n",
    "     # Remove each defined prefix from the string\n",
    "    for substring in substrings_to_delete:\n",
    "        address_str = address_str.replace(substring, \"\")\n",
    "\n",
    "    # Remove multiple spaces\n",
    "    address_str = \" \".join(address_str.split())\n",
    "    \n",
    "    return address_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location(address_str, retries=3):\n",
    "    \n",
    "    \"\"\"\n",
    "    Attempts to geocode a given address string using the Nominatim geolocation service,\n",
    "    applying several variations of the address format to improve matching success.\n",
    "\n",
    "    This function cleans the input address, generates possible simplified versions,\n",
    "    and tries to geocode each of them. It's especially useful for handling inconsistent\n",
    "    or verbose address formats (e.g., from real-world datasets).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    address_str : str\n",
    "        The full address string to geocode.\n",
    "        \n",
    "    retries : int, optional (default=3)\n",
    "        Number of retry attempts in case of timeouts from the geolocation service.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple or None\n",
    "        Returns a tuple (latitude, longitude) if the address is successfully geocoded,\n",
    "        otherwise returns None if all attempts fail.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize Nominatim geocoder\n",
    "    geolocator = Nominatim(user_agent=\"geo_folium_app\")\n",
    "    \n",
    "    # Clean address\n",
    "    original_address = prepare_address_string(address_str)\n",
    "    \n",
    "    # List of address variations to try\n",
    "    address_variations = [original_address]\n",
    "\n",
    "    # Attempt to extract city name based on capitalized words (heuristic)o\n",
    "    city_name = re.findall(r'\\b[A-ZŚĆŁŹŻĄĘÓŃ][a-ząćęłńóśźżA-ZŚĆŁŹŻĄĘÓŃ]*\\b', original_address)\n",
    "    city_name = city_name[-1] if city_name else \"\"\n",
    "    \n",
    "    # Generate address variations if the address contains multiple comma-separated parts\n",
    "    parts = original_address.split(\", \")\n",
    "    if len(parts) > 2:\n",
    "        for i in range(len(parts)-1):\n",
    "            street_name = parts[i]\n",
    "            address_variations.append(f\"{street_name}, {city_name}\")\n",
    "        \n",
    "            # Handle cases where streets contain multiple names separated by slashes\n",
    "            pattern = r\"(?<=\\b\\w)/(?=\\w\\b)\"\n",
    "            if re.search(pattern, parts[i]):\n",
    "                street_name = parts[i].split(\"/\")[0]\n",
    "                address_variations.append(f\"{street_name}, {city_name}\")\n",
    "            \n",
    "             # Attempt simplification by using only the last word of a long street name\n",
    "            if (len(parts[i])>20 and \" \" in parts[i]):\n",
    "                street_name = parts[i].split(\" \")[-1]\n",
    "                address_variations.append(f\"{street_name}, {city_name}\")\n",
    "    \n",
    "    # Add a fallback using city and last part (e.g., region)\n",
    "    address_variations.append(f\"{city_name}, {parts[-1]}\") \n",
    "\n",
    "    # Try geocoding each variation with retry logic\n",
    "    for attempt in range(retries):\n",
    "        for address in address_variations:\n",
    "            try:\n",
    "                location = geolocator.geocode(address)\n",
    "                if location:\n",
    "                    return location.latitude, location.longitude\n",
    "            except GeocoderTimedOut:\n",
    "                print(f\"Timeout for address: {address}, retrying...\")\n",
    "\n",
    "    print(\"The address could not be found\")\n",
    "    \n",
    "    return None  # Return None if all attempts fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it was checked if for the first 100 addresses it was possible to find addresses\n",
    "for i in range(100):\n",
    "    get_location(data_initial['address'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_location(address_str):\n",
    "    \n",
    "    \"\"\"\n",
    "    Visualizes the geographic location of a given address using Folium.\n",
    "\n",
    "    This function first attempts to geocode the provided address string using the\n",
    "    `get_location` function (which internally uses Nominatim). If a valid location\n",
    "    is found, it renders an interactive map with a marker pointing to the location.\n",
    "\n",
    "    It is particularly useful for quickly validating address resolution and\n",
    "    visualizing spatial data in a Jupyter notebook environment.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    address_str : str\n",
    "        A textual representation of the address to geocode and map.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    folium.Map or None\n",
    "        A Folium map object with a marker at the resolved location if successful;\n",
    "        otherwise, prints a message and returns None.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize geocoder\n",
    "    geolocator = Nominatim(user_agent=\"geo_folium_app\")\n",
    "\n",
    "    # Get location\n",
    "    location = get_location(address_str)\n",
    "\n",
    "    # Check if location was found\n",
    "    if location:\n",
    "        lat, lon = location[0], location[1]\n",
    "\n",
    "        # Create a folium map centered at the location\n",
    "        map_obj = folium.Map(location=[lat, lon], zoom_start=14)\n",
    "\n",
    "        # Add a marker for the location\n",
    "        folium.Marker([lat, lon],\n",
    "                      popup = address_str,\n",
    "                      tooltip=\"Click for info\").add_to(map_obj)\n",
    "\n",
    "        # Display map inline in the notebook\n",
    "        return map_obj\n",
    "\n",
    "    else:\n",
    "        print(\"Location not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
       "&lt;html&gt;\n",
       "&lt;head&gt;\n",
       "    \n",
       "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
       "    \n",
       "        &lt;script&gt;\n",
       "            L_NO_TOUCH = false;\n",
       "            L_DISABLE_3D = false;\n",
       "        &lt;/script&gt;\n",
       "    \n",
       "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
       "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://code.jquery.com/jquery-3.7.1.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap-glyphicons.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
       "    \n",
       "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
       "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
       "            &lt;style&gt;\n",
       "                #map_a8e90462b4c6e8d53f78a34b4a177265 {\n",
       "                    position: relative;\n",
       "                    width: 100.0%;\n",
       "                    height: 100.0%;\n",
       "                    left: 0.0%;\n",
       "                    top: 0.0%;\n",
       "                }\n",
       "                .leaflet-container { font-size: 1rem; }\n",
       "            &lt;/style&gt;\n",
       "        \n",
       "&lt;/head&gt;\n",
       "&lt;body&gt;\n",
       "    \n",
       "    \n",
       "            &lt;div class=&quot;folium-map&quot; id=&quot;map_a8e90462b4c6e8d53f78a34b4a177265&quot; &gt;&lt;/div&gt;\n",
       "        \n",
       "&lt;/body&gt;\n",
       "&lt;script&gt;\n",
       "    \n",
       "    \n",
       "            var map_a8e90462b4c6e8d53f78a34b4a177265 = L.map(\n",
       "                &quot;map_a8e90462b4c6e8d53f78a34b4a177265&quot;,\n",
       "                {\n",
       "                    center: [53.0523879, 18.6022106],\n",
       "                    crs: L.CRS.EPSG3857,\n",
       "                    ...{\n",
       "  &quot;zoom&quot;: 14,\n",
       "  &quot;zoomControl&quot;: true,\n",
       "  &quot;preferCanvas&quot;: false,\n",
       "}\n",
       "\n",
       "                }\n",
       "            );\n",
       "\n",
       "            \n",
       "\n",
       "        \n",
       "    \n",
       "            var tile_layer_68c05d4a43de32b56eeeae989835c8eb = L.tileLayer(\n",
       "                &quot;https://tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
       "                {\n",
       "  &quot;minZoom&quot;: 0,\n",
       "  &quot;maxZoom&quot;: 19,\n",
       "  &quot;maxNativeZoom&quot;: 19,\n",
       "  &quot;noWrap&quot;: false,\n",
       "  &quot;attribution&quot;: &quot;\\u0026copy; \\u003ca href=\\&quot;https://www.openstreetmap.org/copyright\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e contributors&quot;,\n",
       "  &quot;subdomains&quot;: &quot;abc&quot;,\n",
       "  &quot;detectRetina&quot;: false,\n",
       "  &quot;tms&quot;: false,\n",
       "  &quot;opacity&quot;: 1,\n",
       "}\n",
       "\n",
       "            );\n",
       "        \n",
       "    \n",
       "            tile_layer_68c05d4a43de32b56eeeae989835c8eb.addTo(map_a8e90462b4c6e8d53f78a34b4a177265);\n",
       "        \n",
       "    \n",
       "            var marker_c670b76d088707f60c373f4328fbcba6 = L.marker(\n",
       "                [53.0523879, 18.6022106],\n",
       "                {\n",
       "}\n",
       "            ).addTo(map_a8e90462b4c6e8d53f78a34b4a177265);\n",
       "        \n",
       "    \n",
       "        var popup_b7ef4608998078a45009bc569d635465 = L.popup({\n",
       "  &quot;maxWidth&quot;: &quot;100%&quot;,\n",
       "});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_795b5534fc8ba715e8e539218e10b307 = $(`&lt;div id=&quot;html_795b5534fc8ba715e8e539218e10b307&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;ul. Henryka Strobanda, Wrzosy, Toruń, kujawsko-pomorskie&lt;/div&gt;`)[0];\n",
       "                popup_b7ef4608998078a45009bc569d635465.setContent(html_795b5534fc8ba715e8e539218e10b307);\n",
       "            \n",
       "        \n",
       "\n",
       "        marker_c670b76d088707f60c373f4328fbcba6.bindPopup(popup_b7ef4608998078a45009bc569d635465)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "    \n",
       "            marker_c670b76d088707f60c373f4328fbcba6.bindTooltip(\n",
       "                `&lt;div&gt;\n",
       "                     Click for info\n",
       "                 &lt;/div&gt;`,\n",
       "                {\n",
       "  &quot;sticky&quot;: true,\n",
       "}\n",
       "            );\n",
       "        \n",
       "&lt;/script&gt;\n",
       "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x1b9ba61bbb0>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_location(data_initial.iloc[0,][\"address\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
