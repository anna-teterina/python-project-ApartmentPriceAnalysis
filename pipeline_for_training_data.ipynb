{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34b9668c",
   "metadata": {},
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0ebb35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# for load the data pre-preparation function\n",
    "import importlib.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79c253e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=False)\n",
    "\n",
    "work_dir = r'C:\\Users\\krasavica\\Desktop\\Projekty - DS\\python-project-ApartmentPriceAnalysis'\n",
    "os.chdir(work_dir)\n",
    "\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a32d5c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading functions from the file\n",
    "module_name = f\"pipeline_pre-processing\"\n",
    "module_path = f\"pipeline_pre-processing.py\"\n",
    "\n",
    "spec = importlib.util.spec_from_file_location(module_name, module_path)\n",
    "module = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78b8f56",
   "metadata": {},
   "source": [
    "# Functions for training finale model\n",
    "Before modeling, the data were processed as follows:\n",
    "1. categorical features were one-hot encoded - **one_hot_encode_train_ecoder**\n",
    "2. all features were scale with z-score normalization - **scale_train_scaler**\n",
    "3. missing data was filled usin kNN imputation - **kNN_impute_train_imputer**\n",
    "4. all the steps above were combined, and the fitted encoder, scaler and imputer were saved - **prepare_final_training_data**\n",
    "5. additional data modification to improve model predictions for low-cost and high-priced housing groups (additional explanation in model_choosing_analysis file) - **modify_train_data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5212274",
   "metadata": {},
   "source": [
    "### one_hot_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21949d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_train_encoder(X_to_encode, features_to_onehotencode):\n",
    "    \n",
    "    \"\"\"\n",
    "    Applies one-hot encoding to specified categorical features in the training dataset\n",
    "    and returns both the transformed DataFrame and a dictionary of trained encoders.\n",
    "\n",
    "    For each categorical feature, the function:\n",
    "    - Checks whether specific unwanted categories (e.g., 'nie podano') are present.\n",
    "    - Drops the first matching category from the one-hot encoding if found, otherwise drops the first by default.\n",
    "    - Uses sklearn's OneHotEncoder with 'handle_unknown=ignore' to handle unseen categories gracefully.\n",
    "    - Returns a transformed DataFrame and a dictionary of fitted encoders for future use (e.g., for encoding test data).\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_to_encode : pd.DataFrame\n",
    "        Input DataFrame containing categorical features to be encoded.\n",
    "\n",
    "    features_to_onehotencode : list of str\n",
    "        List of column names in `X_to_encode` to apply one-hot encoding on.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X : pd.DataFrame\n",
    "        DataFrame with original categorical features replaced by one-hot encoded columns.\n",
    "\n",
    "    encoders : dict\n",
    "        Dictionary mapping each encoded feature name to its corresponding fitted OneHotEncoder instance.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create copy of the original dataset to avoid modifying it in-place\n",
    "    X = X_to_encode.copy()\n",
    "    \n",
    "    # Dictionary to store fitted encoders for each column\n",
    "    encoders = {}\n",
    "    \n",
    "    for column in features_to_onehotencode:\n",
    "        # Define categories that should be dropped if they exist\n",
    "        categories_to_drop = ['nie podano', 'inny', 'wt√≥rny', 'prywatny', 'opolskie']\n",
    "        \n",
    "        # Determine which category to drop (first match found)\n",
    "        cat_to_drop = None\n",
    "        \n",
    "        for cat in categories_to_drop:\n",
    "            if cat in X[column].unique():\n",
    "                cat_to_drop = [cat]\n",
    "                break\n",
    "                \n",
    "        # Create and fit the encoder\n",
    "        encoder = OneHotEncoder(drop = cat_to_drop if cat_to_drop else 'first',\n",
    "                                sparse_output = False, handle_unknown = 'ignore')\n",
    "        encoder.fit(X[[column]])\n",
    "        \n",
    "        # Get names of one-hot encoded columns\n",
    "        cols = encoder.get_feature_names_out([column])\n",
    "        \n",
    "        # Transform the data and wrap in DataFrame\n",
    "        X_encoded = pd.DataFrame(encoder.transform(X[[column]]), columns=cols, index=X.index)\n",
    "        \n",
    "        # Append the encoded columns to the dataset\n",
    "        X = pd.concat([X, X_encoded], axis=1)\n",
    "        \n",
    "        # Save the encoder for this column\n",
    "        encoders[column] = encoder\n",
    "        \n",
    "    # Drop the original categorical column\n",
    "    X.drop(features_to_onehotencode, axis=1, inplace=True)\n",
    "    \n",
    "    return X, encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f135014",
   "metadata": {},
   "source": [
    "### scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42ae1258",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_train_scaler(X_to_scale):\n",
    "    \n",
    "    \"\"\"\n",
    "    Applies standard scaling (zero mean, unit variance) to the input dataset and returns\n",
    "    both the scaled dataset and the fitted scaler for future use (e.g., for test data).\n",
    "\n",
    "    The function performs the following steps:\n",
    "    - Copies the input DataFrame to avoid modifying the original.\n",
    "    - Fits a 'StandardScaler' from 'sklearn.preprocessing' to the data.\n",
    "    - Applies the transformation and returns the scaled data along with the scaler.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_to_scale : pd.DataFrame\n",
    "        The DataFrame containing numerical features to be scaled.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X : pd.DataFrame\n",
    "        The scaled version of the input DataFrame, with the same column names.\n",
    "\n",
    "    scaler : StandardScaler\n",
    "        The fitted 'StandardScaler' object, which can be used to transform new data\n",
    "        (e.g., test set) using the same scaling parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create copy of the original dataset to avoid modifying it in-place\n",
    "    X = X_to_scale.copy()\n",
    "    \n",
    "    # Create the scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit and apply the scaler to the data and preserve column names\n",
    "    X = pd.DataFrame(scaler.fit_transform(X), columns = X.columns)\n",
    "        \n",
    "    return X, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e2f388",
   "metadata": {},
   "source": [
    "### kNN_impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "836256f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kNN_impute_train_imputer(X_to_impute, features_for_kNN_impute, features_to_onehotencode):       \n",
    "    \n",
    "    \"\"\"\n",
    "    Performs K-Nearest Neighbors (KNN) imputation on selected features of a dataset,\n",
    "    including those that have been one-hot encoded.\n",
    "\n",
    "    The function:\n",
    "    - Identifies features that were originally categorical and have been one-hot encoded.\n",
    "    - Replaces these original variables in the list of features to impute with their\n",
    "      one-hot encoded column names.\n",
    "    - Fits a 'KNNImputer' on the specified features.\n",
    "    - Returns the dataset with imputed values and a dictionary containing the fitted\n",
    "      imputer and the actual list of features used.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_to_impute : pd.DataFrame\n",
    "        The input dataset (typically already encoded and scaled) with missing values to be imputed.\n",
    "\n",
    "    features_for_kNN_impute : list of str\n",
    "        List of original features intended for imputation.\n",
    "\n",
    "    features_to_onehotencode : list of str\n",
    "        List of features that were one-hot encoded (to help expand into multiple columns).\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X : pd.DataFrame\n",
    "        The dataset with imputed values for the specified features.\n",
    "\n",
    "    imputer_dict : dict\n",
    "        A dictionary containing:\n",
    "            - 'imputer' : the fitted 'KNNImputer'`object\n",
    "            - 'features': the actual list of column names used for imputation,\n",
    "                          including one-hot encoded columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create copy of the original dataset to avoid modifying it in-place\n",
    "    X = X_to_impute.copy()\n",
    "    \n",
    "     # Copy the list of features to impute\n",
    "    features_for_kNN_impute_new = features_for_kNN_impute.copy()\n",
    "    \n",
    "    # Identify one-hot encoded variables among features to impute\n",
    "    encoded_vars = list(set(features_for_kNN_impute) & set(features_to_onehotencode))\n",
    "    \n",
    "    # Replace encoded variables with their actual one-hot encoded column names\n",
    "    for var in encoded_vars:\n",
    "        col_after_encode = [col for col in X.columns if col.startswith(var)]\n",
    "        features_for_kNN_impute_new.remove(var)\n",
    "        features_for_kNN_impute_new = features_for_kNN_impute_new + col_after_encode\n",
    "        \n",
    "    # Create the imputer\n",
    "    imputer = KNNImputer()\n",
    "    \n",
    "    # Fit and apply imputation only on the selected columns\n",
    "    X[features_for_kNN_impute_new] = imputer.fit_transform(X[features_for_kNN_impute_new])\n",
    "    \n",
    "    # Return the transformed data and the fitted imputer with the column names used\n",
    "    imputer_dict = {'imputer': imputer,\n",
    "                    'features': features_for_kNN_impute_new}\n",
    "    \n",
    "    return X, imputer_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8640a421",
   "metadata": {},
   "source": [
    "### prepare_final_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4629ee3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_final_training_data(data_to_train, to_save = True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Prepares the final training dataset by applying a complete preprocessing pipeline,\n",
    "    including one-hot encoding, scaling, and KNN imputation.\n",
    "\n",
    "    This function:\n",
    "    - Loads the model configuration (features, encoders) from a predefined dictionary.\n",
    "    - Applies preprocessing steps in the following order:\n",
    "        1. Cleans missing and outlier values.\n",
    "        2. One-hot encoding of categorical variables.\n",
    "        3. Feature standardization (z-score scaling).\n",
    "        4. Missing data imputation using K-Nearest Neighbors.\n",
    "    - Optionally saves the fitted preprocessing objects (encoder, scaler, imputer) for future use.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        The raw input dataset containing both features and the target variable.\n",
    "\n",
    "    to_save : bool, default=True\n",
    "        If True, saves the fitted encoder, scaler, and imputer to disk under the\n",
    "        folder '1. Data Preparation/pipeline_objects'.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X_imputed : pd.DataFrame\n",
    "        The fully preprocessed feature matrix ready for training.\n",
    "\n",
    "    y : pd.Series\n",
    "        The target variable extracted from the input data.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the list of features and settings used for preprocessing\n",
    "    model_features_dict = joblib.load(\"1. Data Preparation/model_features_dict.joblib\")\n",
    "    \n",
    "    features_to_use = model_features_dict['features_to_use']\n",
    "    target = model_features_dict['target']\n",
    "    features_to_onehotencode = model_features_dict['features_to_onehotencode']\n",
    "    features_for_kNN_impute = model_features_dict['features_for_kNN_impute']\n",
    "    \n",
    "    # Check whether all required features are present in the data\n",
    "    if all(x in data_to_train.columns for x in model_features_dict['features_to_use']) == 0:\n",
    "        print(\"The dataset does not have all the variables defined for modeling\")\n",
    "    \n",
    "    # Copy the dataset to avoid in-place changes\n",
    "    data = data_to_train.copy()\n",
    "    \n",
    "    # Proceed missing and outlier values\n",
    "    data = module.cleaning_data(data, train_dataset = True)\n",
    "    \n",
    "    # Ensure compatibility with np.nan\n",
    "    data = data.replace({pd.NA: np.nan})\n",
    "    \n",
    "    # Separate input features and target\n",
    "    X = data[features_to_use]\n",
    "    y = data[target]\n",
    "    \n",
    "    # One-hot encode categorical features\n",
    "    X_encoded, encoder = one_hot_encode_train_encoder(X, features_to_onehotencode)\n",
    "    \n",
    "    # Scale features using StandardScaler\n",
    "    X_scaled, scaler = scale_train_scaler(X_encoded)\n",
    "    \n",
    "    # Impute missing values using KNN imputer\n",
    "    X_imputed, imputer = kNN_impute_train_imputer(X_scaled, features_for_kNN_impute, features_to_onehotencode)\n",
    "    \n",
    "    # Optionally save preprocessing objects to disk for future use\n",
    "    if to_save: \n",
    "        joblib.dump(encoder, \"production_pipeline_objects/encoder.pkl\")\n",
    "        joblib.dump(scaler, \"production_pipeline_objects/scaler.pkl\")\n",
    "        joblib.dump(imputer, \"production_pipeline_objects/imputer.pkl\")\n",
    "    \n",
    "    return X_imputed, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeb2f52",
   "metadata": {},
   "source": [
    "## modify_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5196f786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_train_data(X_train, y_train):\n",
    "    \n",
    "    \"\"\"\n",
    "    Modify the target variable 'y_train' based on specified lower and upper thresholds \n",
    "    and associated multiplicative weights.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : pandas.DataFrame or numpy.ndarray\n",
    "        Feature matrix used for training.\n",
    "    \n",
    "    y_train : pandas.Series or numpy.ndarray\n",
    "        Target vector used for training.\n",
    "\n",
    "    lower_thresholds : list of float, optional (default = [0])\n",
    "        Threshold values below which the corresponding lower_wages will be applied.\n",
    "        Each threshold defines a cutoff, and values below it will be multiplied.\n",
    "\n",
    "    lower_wages : list of float, optional (default = [1])\n",
    "        Multiplicative weights applied to target values below each corresponding\n",
    "        value in 'lower_thresholds'.\n",
    "\n",
    "    upper_thresholds : list of float, optional (default = [1_500_000])\n",
    "        Threshold values above which the corresponding upper_wages will be applied.\n",
    "        Each threshold defines a cutoff, and values above it will be multiplied.\n",
    "\n",
    "    upper_wages : list of float, optional (default = [1])\n",
    "        Multiplicative weights applied to target values above each corresponding\n",
    "        value in 'upper_thresholds'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train : unchanged\n",
    "        The input features passed through without modification.\n",
    "\n",
    "    y_train_copy : pandas.Series or numpy.ndarray\n",
    "        A modified version of `y_train`, where values below/above the defined \n",
    "        thresholds have been scaled accordingly.\n",
    "\n",
    "    \"\"\"\n",
    "    modification_settings = joblib.load(\"2. Model Building/final_modification_settings.joblib\")\n",
    "    \n",
    "    lower_thresholds = modification_settings[\"lower_thresholds\"]\n",
    "    lower_wages = modification_settings[\"lower_wages\"]\n",
    "    upper_thresholds = modification_settings[\"upper_thresholds\"]\n",
    "    upper_wages = modification_settings[\"upper_wages\"]\n",
    "\n",
    "    # Create a copy of y_train to avoid modifying the original target data\n",
    "    y_train_copy = y_train.copy()\n",
    "    \n",
    "    # Apply scaling to values below the given lower thresholds\n",
    "    for i in range(len(lower_thresholds)):\n",
    "        condition = y_train_copy < lower_thresholds[i]\n",
    "        y_train_copy[condition] = y_train_copy[condition] * lower_wages[i]\n",
    "\n",
    "    # Apply scaling to values above the given upper thresholds\n",
    "    for i in range(len(upper_thresholds)):\n",
    "        condition = y_train_copy > upper_thresholds[i]\n",
    "        y_train_copy[condition] = y_train_copy[condition] * upper_wages[i]\n",
    "\n",
    "    return X_train, y_train_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03362b35",
   "metadata": {},
   "source": [
    "# Training final model and saving pipeline elements for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "660d64d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the categorizations occurring in the set in multi-vector selection variables were coded.\n"
     ]
    }
   ],
   "source": [
    "data_initial = pd.read_csv('data_2024-01.csv', index_col = 0)\n",
    "data_for_analysis = module.preliminary_transform(data_initial, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a64ceb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Application of the function\n",
    "X, y = prepare_final_training_data(data_for_analysis, to_save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f075cce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the final model on parameters choosed in model_choosing_analysis file\n",
    "model_params = joblib.load(\"2. Model Building/final_model_params.joblib\")\n",
    "\n",
    "X_modified, y_modified = modify_train_data(X, y)\n",
    "\n",
    "trained_model = model_params.fit(X_modified, y_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df55a1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['production_pipeline_objects/trained_model.joblib']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Savining trained model\n",
    "joblib.dump(trained_model, \"production_pipeline_objects/trained_model.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4a5cc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the base value (mean) for SHAP report\n",
    "shap_base_value = np.mean(y, axis=None)\n",
    "shap_base_value_df = pd.DataFrame([shap_base_value],\n",
    "                                   columns = [\"base_value\"])\n",
    "shap_base_value_df.to_csv(\"production_pipeline_objects/shap_base_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c970024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the transformed data for testing the SHAP report\n",
    "X.to_csv('3. Feature importance and report/feature_imp_data_X.csv')\n",
    "pd.DataFrame(y, columns = ['price']).to_csv('3. Feature importance and report/feature_imp_data_y.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
